{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93057,"databundleVersionId":11145869,"isSourceIdPinned":false,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3952b329","cell_type":"markdown","source":"# PyTorch Custom ResNet for CIFAR-10 on Apple MPS\n\nThis notebook implements a custom ResNet model for CIFAR-10 with a flexible architecture and Apple MPS support. It includes:\n\n- **Custom ResNet Model:** Definition of the ResNet model and helper functions.\n- **Data Loading:** Functions to download and preprocess CIFAR-10.\n- **Custom Test Dataset:** A dataset class to load unlabeled CIFAR test images from a pickle file.\n- **Training & Testing Functions:** Routines to train and evaluate the model.\n- **Inference:** Code to run inference on the custom test dataset and save predictions.\n\nUse the provided cells to train the model or run inference (make sure you have the test pickle file `data/cifar_test_nolabel.pkl` in place).","metadata":{}},{"id":"imports","cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom PIL import Image","metadata":{},"outputs":[],"execution_count":null},{"id":"custom_resnet","cell_type":"code","source":"# Custom ResNet Model Definitions\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = self.relu(out)\n        return out\n\n\nclass CustomResNet(nn.Module):\n    def __init__(self, blocks_per_stage=[2, 2, 2, 2], base_channels=32, num_classes=10):\n        \"\"\"\n        Args:\n            blocks_per_stage (list): Four integers specifying the number of residual blocks per stage.\n            base_channels (int): Number of channels for the initial convolution.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(CustomResNet, self).__init__()\n        self.in_channels = base_channels\n        # Initial convolution for CIFAR-10 (3-channel, 32x32 images).\n        self.conv1 = nn.Conv2d(3, base_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(base_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.layer1 = self._make_layer(base_channels, blocks_per_stage[0], stride=1)\n        self.layer2 = self._make_layer(base_channels * 2, blocks_per_stage[1], stride=2)\n        self.layer3 = self._make_layer(base_channels * 4, blocks_per_stage[2], stride=2)\n        self.layer4 = self._make_layer(base_channels * 8, blocks_per_stage[3], stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(base_channels * 8, num_classes)\n\n    def _make_layer(self, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n        self.in_channels = out_channels\n        for _ in range(1, num_blocks):\n            layers.append(BasicBlock(out_channels, out_channels, stride=1))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n\ndef count_parameters(model):\n    \"\"\"Return the total number of trainable parameters.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Quick test of the model (uncomment to run)\n# model = CustomResNet(blocks_per_stage=[2, 3, 3, 2], base_channels=32, num_classes=10)\n# print(\"Total parameters:\", count_parameters(model))\n","metadata":{},"outputs":[],"execution_count":null},{"id":"data_loading","cell_type":"code","source":"# Data Loading Function for CIFAR-10\ndef get_data(batch_size=128):\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    return train_loader, test_loader\n","metadata":{},"outputs":[],"execution_count":null},{"id":"custom_test_dataset","cell_type":"code","source":"# Custom Test Dataset for CIFAR (expects a pickle file with unlabeled images)\nclass CIFARTestDataset(Dataset):\n    \"\"\"\n    Custom dataset for CIFAR test data stored in a pickle file.\n    The pickle file is expected to contain a list or array of images.\n    \"\"\"\n    def __init__(self, pkl_file, transform=None):\n        with open(pkl_file, 'rb') as f:\n            self.data = pickle.load(f)\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n            ])\n        else:\n            self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img = self.data[idx]\n        if isinstance(img, np.ndarray):\n            img = img.astype('uint8')\n        if self.transform:\n            img = self.transform(img)\n        return img\n","metadata":{},"outputs":[],"execution_count":null},{"id":"train_test_functions","cell_type":"code","source":"# Training and Testing Functions\ndef train(model, train_loader, optimizer, criterion, device, epoch, print_interval=50):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, targets) in enumerate(train_loader, 1):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if i % print_interval == 0:\n            print(f\"Epoch: {epoch} | Iteration: {i} | Loss: {running_loss / print_interval:.3f}\")\n            running_loss = 0.0\n\ndef test(model, test_loader, device, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n    accuracy = correct / total\n    print(f\"Epoch {epoch}: Test accuracy: {accuracy:.3f}\")\n    return accuracy\n","metadata":{},"outputs":[],"execution_count":null},{"id":"device_and_model","cell_type":"code","source":"# Set device: Use Apple MPS if available, otherwise CUDA or CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nprint(\"Using device:\", device)\n\n# Instantiate the model and print the parameter count\nmodel = CustomResNet(blocks_per_stage=[2, 3, 3, 2], base_channels=32, num_classes=10).to(device)\nprint(\"Total parameters:\", count_parameters(model))\n","metadata":{},"outputs":[],"execution_count":null},{"id":"training_mode","cell_type":"code","source":"Training Mode\nUncomment the code below to train the model on CIFAR-10\n\ntrain_loader, test_loader = get_data(batch_size=128)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\nepochs = 50  # Adjust the number of epochs as needed\nfor epoch in range(1, epochs + 1):\n    train(model, train_loader, optimizer, criterion, device, epoch)\n    test(model, test_loader, device, epoch)\n# Save the trained model checkpoint\ntorch.save(model.state_dict(), \"model_checkpoint.pth\")\nprint(\"Model checkpoint saved to model_checkpoint.pth\")\n","metadata":{},"outputs":[],"execution_count":null},{"id":"inference_mode","cell_type":"code","source":"# Inference Mode\n# This cell loads a saved model checkpoint (if available) and runs inference on the custom test dataset.\n\nif os.path.exists(\"model_checkpoint.pth\"):\n    model.load_state_dict(torch.load(\"model_checkpoint.pth\", map_location=device))\n    print(\"Loaded model checkpoint from model_checkpoint.pth\")\nelse:\n    print(\"No checkpoint found. Running inference with untrained model.\")\n\ntest_pkl = \"data/cifar_test_nolabel.pkl\"\nif os.path.exists(test_pkl):\n    test_dataset = CIFARTestDataset(test_pkl)\n    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for images in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy().tolist())\n    with open(\"cifar_test_predictions.pkl\", \"wb\") as f:\n        pickle.dump(predictions, f)\n    print(\"Predictions saved to cifar_test_predictions.pkl\")\nelse:\n    print(f\"Test file {test_pkl} not found.\")\n","metadata":{},"outputs":[],"execution_count":null},{"id":"02125e95","cell_type":"markdown","source":"## Instructions\n\n1. **Training Mode:** Uncomment and run the **Training Mode** cell to train the model. The trained model will be saved as `model_checkpoint.pth`.\n2. **Inference Mode:** Ensure that your test pickle file (`data/cifar_test_nolabel.pkl`) is available, then run the **Inference Mode** cell to generate predictions. The predictions will be saved to `cifar_test_predictions.pkl`.\n\nAdjust parameters (such as number of epochs) as needed.","metadata":{}}]}