{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":93057,"databundleVersionId":11145869,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2fa738dd-1c6c-4248-b7c6-5c2119398dbe","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:03:05.219070Z","iopub.execute_input":"2025-03-02T11:03:05.219413Z","iopub.status.idle":"2025-03-02T11:03:05.540284Z","shell.execute_reply.started":"2025-03-02T11:03:05.219381Z","shell.execute_reply":"2025-03-02T11:03:05.539534Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_1\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_2\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/batches.meta\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/test_batch\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_3\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_5\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_4\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/readme.html\n","output_type":"stream"}],"execution_count":1},{"id":"3a6dc75f-80e2-47c5-886b-f2b80467eb24","cell_type":"markdown","source":"# Custom ResNet for CIFAR-10 on CUDA\n\nThis notebook implements a custom ResNet model for CIFAR-10 using PyTorch. It includes two major sections:\n\n1. **Training Section:** Loads the CIFAR-10 training and test sets via torchvision, trains the model, and saves a checkpoint.\n\n2. **Inference Section:** Loads a test batch from a pickle file (with no labels), runs inference using the trained model (if available), and saves the predictions to a CSV file.\n\nMake sure the test pickle file is located at `/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl` for inference.","metadata":{}},{"id":"imports","cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport pandas as pd\n\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:03:05.541417Z","iopub.execute_input":"2025-03-02T11:03:05.541740Z","iopub.status.idle":"2025-03-02T11:03:08.133828Z","shell.execute_reply.started":"2025-03-02T11:03:05.541719Z","shell.execute_reply":"2025-03-02T11:03:08.133129Z"}},"outputs":[],"execution_count":2},{"id":"model_definition","cell_type":"code","source":"# Model Definitions\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = self.relu(out)\n        return out\n\n\nclass CustomResNet(nn.Module):\n    def __init__(self, blocks_per_stage=[3, 4, 4, 3], base_channels=32, num_classes=10):\n        \"\"\"\n        Args:\n            blocks_per_stage (list): Four integers specifying the number of residual blocks per stage.\n            base_channels (int): Number of channels for the initial convolution.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(CustomResNet, self).__init__()\n        self.in_channels = base_channels\n        # Initial convolution for CIFAR-10 (3-channel, 32x32 images)\n        self.conv1 = nn.Conv2d(3, base_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(base_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.layer1 = self._make_layer(base_channels, blocks_per_stage[0], stride=1)\n        self.layer2 = self._make_layer(base_channels * 2, blocks_per_stage[1], stride=2)\n        self.layer3 = self._make_layer(base_channels * 4, blocks_per_stage[2], stride=2)\n        self.layer4 = self._make_layer(base_channels * 8, blocks_per_stage[3], stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(base_channels * 8, num_classes)\n\n    def _make_layer(self, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n        self.in_channels = out_channels\n        for _ in range(1, num_blocks):\n            layers.append(BasicBlock(out_channels, out_channels, stride=1))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:03:08.135148Z","iopub.execute_input":"2025-03-02T11:03:08.135574Z","iopub.status.idle":"2025-03-02T11:03:08.146547Z","shell.execute_reply.started":"2025-03-02T11:03:08.135550Z","shell.execute_reply":"2025-03-02T11:03:08.145714Z"}},"outputs":[],"execution_count":3},{"id":"data_loading_function","cell_type":"code","source":"# Data Loading Function for CIFAR-10 using torchvision\ndef get_data(batch_size=128):\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    return train_loader, test_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:03:08.148085Z","iopub.execute_input":"2025-03-02T11:03:08.148317Z","iopub.status.idle":"2025-03-02T11:03:08.170409Z","shell.execute_reply.started":"2025-03-02T11:03:08.148275Z","shell.execute_reply":"2025-03-02T11:03:08.169719Z"}},"outputs":[],"execution_count":4},{"id":"training_testing_functions","cell_type":"code","source":"# Training and Testing Functions\ndef train(model, train_loader, optimizer, criterion, device, epoch, print_interval=50):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, targets) in enumerate(train_loader, 1):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if i % print_interval == 0:\n            print(f\"Epoch: {epoch} | Iteration: {i} | Loss: {running_loss / print_interval:.3f}\")\n            running_loss = 0.0\n\ndef test(model, test_loader, device, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n    accuracy = correct / total\n    print(f\"Epoch {epoch}: Test accuracy: {accuracy:.3f}\")\n    return accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:03:08.171240Z","iopub.execute_input":"2025-03-02T11:03:08.171556Z","iopub.status.idle":"2025-03-02T11:03:08.186678Z","shell.execute_reply.started":"2025-03-02T11:03:08.171525Z","shell.execute_reply":"2025-03-02T11:03:08.185962Z"}},"outputs":[],"execution_count":5},{"id":"device_model_init","cell_type":"code","source":"# Set device: Use Apple MPS if available, otherwise CUDA or CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nprint(\"Using device:\", device)\n\n# Instantiate the model and print the parameter count\nmodel = CustomResNet(blocks_per_stage=[3, 4, 4, 3], base_channels=32, num_classes=10).to(device)\nprint(\"Total parameters:\", count_parameters(model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:03:08.187396Z","iopub.execute_input":"2025-03-02T11:03:08.187588Z","iopub.status.idle":"2025-03-02T11:03:08.388662Z","shell.execute_reply.started":"2025-03-02T11:03:08.187571Z","shell.execute_reply":"2025-03-02T11:03:08.387707Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTotal parameters: 3167018\n","output_type":"stream"}],"execution_count":6},{"id":"training_mode_cell","cell_type":"code","source":"# Training Mode\n# This cell trains the model on CIFAR-10 and saves a checkpoint.\n\ntrain_loader, test_loader = get_data(batch_size=128)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\nepochs = 50  # Adjust epochs as needed\n\nfor epoch in range(1, epochs + 1):\n    train(model, train_loader, optimizer, criterion, device, epoch)\n    test(model, test_loader, device, epoch)\n\n# Save the trained model checkpoint\ntorch.save(model.state_dict(), \"model_checkpoint.pth\")\nprint(\"Model checkpoint saved to model_checkpoint.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:03:08.389579Z","iopub.execute_input":"2025-03-02T11:03:08.389825Z","iopub.status.idle":"2025-03-02T11:19:55.078153Z","shell.execute_reply.started":"2025-03-02T11:03:08.389805Z","shell.execute_reply":"2025-03-02T11:19:55.077082Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEpoch: 1 | Iteration: 50 | Loss: 1.900\nEpoch: 1 | Iteration: 100 | Loss: 1.667\nEpoch: 1 | Iteration: 150 | Loss: 1.536\nEpoch: 1 | Iteration: 200 | Loss: 1.496\nEpoch: 1 | Iteration: 250 | Loss: 1.401\nEpoch: 1 | Iteration: 300 | Loss: 1.267\nEpoch: 1 | Iteration: 350 | Loss: 1.213\nEpoch 1: Test accuracy: 0.553\nEpoch: 2 | Iteration: 50 | Loss: 1.087\nEpoch: 2 | Iteration: 100 | Loss: 1.049\nEpoch: 2 | Iteration: 150 | Loss: 1.012\nEpoch: 2 | Iteration: 200 | Loss: 0.983\nEpoch: 2 | Iteration: 250 | Loss: 0.935\nEpoch: 2 | Iteration: 300 | Loss: 0.903\nEpoch: 2 | Iteration: 350 | Loss: 0.862\nEpoch 2: Test accuracy: 0.673\nEpoch: 3 | Iteration: 50 | Loss: 0.824\nEpoch: 3 | Iteration: 100 | Loss: 0.794\nEpoch: 3 | Iteration: 150 | Loss: 0.787\nEpoch: 3 | Iteration: 200 | Loss: 0.758\nEpoch: 3 | Iteration: 250 | Loss: 0.740\nEpoch: 3 | Iteration: 300 | Loss: 0.719\nEpoch: 3 | Iteration: 350 | Loss: 0.726\nEpoch 3: Test accuracy: 0.760\nEpoch: 4 | Iteration: 50 | Loss: 0.636\nEpoch: 4 | Iteration: 100 | Loss: 0.647\nEpoch: 4 | Iteration: 150 | Loss: 0.654\nEpoch: 4 | Iteration: 200 | Loss: 0.640\nEpoch: 4 | Iteration: 250 | Loss: 0.656\nEpoch: 4 | Iteration: 300 | Loss: 0.618\nEpoch: 4 | Iteration: 350 | Loss: 0.606\nEpoch 4: Test accuracy: 0.770\nEpoch: 5 | Iteration: 50 | Loss: 0.589\nEpoch: 5 | Iteration: 100 | Loss: 0.559\nEpoch: 5 | Iteration: 150 | Loss: 0.578\nEpoch: 5 | Iteration: 200 | Loss: 0.547\nEpoch: 5 | Iteration: 250 | Loss: 0.542\nEpoch: 5 | Iteration: 300 | Loss: 0.548\nEpoch: 5 | Iteration: 350 | Loss: 0.566\nEpoch 5: Test accuracy: 0.772\nEpoch: 6 | Iteration: 50 | Loss: 0.521\nEpoch: 6 | Iteration: 100 | Loss: 0.507\nEpoch: 6 | Iteration: 150 | Loss: 0.517\nEpoch: 6 | Iteration: 200 | Loss: 0.510\nEpoch: 6 | Iteration: 250 | Loss: 0.486\nEpoch: 6 | Iteration: 300 | Loss: 0.512\nEpoch: 6 | Iteration: 350 | Loss: 0.496\nEpoch 6: Test accuracy: 0.823\nEpoch: 7 | Iteration: 50 | Loss: 0.472\nEpoch: 7 | Iteration: 100 | Loss: 0.454\nEpoch: 7 | Iteration: 150 | Loss: 0.464\nEpoch: 7 | Iteration: 200 | Loss: 0.466\nEpoch: 7 | Iteration: 250 | Loss: 0.455\nEpoch: 7 | Iteration: 300 | Loss: 0.463\nEpoch: 7 | Iteration: 350 | Loss: 0.462\nEpoch 7: Test accuracy: 0.822\nEpoch: 8 | Iteration: 50 | Loss: 0.408\nEpoch: 8 | Iteration: 100 | Loss: 0.422\nEpoch: 8 | Iteration: 150 | Loss: 0.398\nEpoch: 8 | Iteration: 200 | Loss: 0.423\nEpoch: 8 | Iteration: 250 | Loss: 0.407\nEpoch: 8 | Iteration: 300 | Loss: 0.412\nEpoch: 8 | Iteration: 350 | Loss: 0.429\nEpoch 8: Test accuracy: 0.819\nEpoch: 9 | Iteration: 50 | Loss: 0.397\nEpoch: 9 | Iteration: 100 | Loss: 0.395\nEpoch: 9 | Iteration: 150 | Loss: 0.377\nEpoch: 9 | Iteration: 200 | Loss: 0.402\nEpoch: 9 | Iteration: 250 | Loss: 0.416\nEpoch: 9 | Iteration: 300 | Loss: 0.396\nEpoch: 9 | Iteration: 350 | Loss: 0.379\nEpoch 9: Test accuracy: 0.836\nEpoch: 10 | Iteration: 50 | Loss: 0.359\nEpoch: 10 | Iteration: 100 | Loss: 0.359\nEpoch: 10 | Iteration: 150 | Loss: 0.366\nEpoch: 10 | Iteration: 200 | Loss: 0.342\nEpoch: 10 | Iteration: 250 | Loss: 0.365\nEpoch: 10 | Iteration: 300 | Loss: 0.369\nEpoch: 10 | Iteration: 350 | Loss: 0.364\nEpoch 10: Test accuracy: 0.856\nEpoch: 11 | Iteration: 50 | Loss: 0.318\nEpoch: 11 | Iteration: 100 | Loss: 0.324\nEpoch: 11 | Iteration: 150 | Loss: 0.316\nEpoch: 11 | Iteration: 200 | Loss: 0.338\nEpoch: 11 | Iteration: 250 | Loss: 0.341\nEpoch: 11 | Iteration: 300 | Loss: 0.337\nEpoch: 11 | Iteration: 350 | Loss: 0.365\nEpoch 11: Test accuracy: 0.865\nEpoch: 12 | Iteration: 50 | Loss: 0.299\nEpoch: 12 | Iteration: 100 | Loss: 0.304\nEpoch: 12 | Iteration: 150 | Loss: 0.320\nEpoch: 12 | Iteration: 200 | Loss: 0.334\nEpoch: 12 | Iteration: 250 | Loss: 0.321\nEpoch: 12 | Iteration: 300 | Loss: 0.312\nEpoch: 12 | Iteration: 350 | Loss: 0.321\nEpoch 12: Test accuracy: 0.858\nEpoch: 13 | Iteration: 50 | Loss: 0.284\nEpoch: 13 | Iteration: 100 | Loss: 0.280\nEpoch: 13 | Iteration: 150 | Loss: 0.311\nEpoch: 13 | Iteration: 200 | Loss: 0.301\nEpoch: 13 | Iteration: 250 | Loss: 0.290\nEpoch: 13 | Iteration: 300 | Loss: 0.309\nEpoch: 13 | Iteration: 350 | Loss: 0.312\nEpoch 13: Test accuracy: 0.876\nEpoch: 14 | Iteration: 50 | Loss: 0.272\nEpoch: 14 | Iteration: 100 | Loss: 0.275\nEpoch: 14 | Iteration: 150 | Loss: 0.287\nEpoch: 14 | Iteration: 200 | Loss: 0.292\nEpoch: 14 | Iteration: 250 | Loss: 0.294\nEpoch: 14 | Iteration: 300 | Loss: 0.270\nEpoch: 14 | Iteration: 350 | Loss: 0.283\nEpoch 14: Test accuracy: 0.847\nEpoch: 15 | Iteration: 50 | Loss: 0.254\nEpoch: 15 | Iteration: 100 | Loss: 0.264\nEpoch: 15 | Iteration: 150 | Loss: 0.254\nEpoch: 15 | Iteration: 200 | Loss: 0.275\nEpoch: 15 | Iteration: 250 | Loss: 0.252\nEpoch: 15 | Iteration: 300 | Loss: 0.268\nEpoch: 15 | Iteration: 350 | Loss: 0.280\nEpoch 15: Test accuracy: 0.870\nEpoch: 16 | Iteration: 50 | Loss: 0.245\nEpoch: 16 | Iteration: 100 | Loss: 0.241\nEpoch: 16 | Iteration: 150 | Loss: 0.240\nEpoch: 16 | Iteration: 200 | Loss: 0.257\nEpoch: 16 | Iteration: 250 | Loss: 0.257\nEpoch: 16 | Iteration: 300 | Loss: 0.247\nEpoch: 16 | Iteration: 350 | Loss: 0.253\nEpoch 16: Test accuracy: 0.869\nEpoch: 17 | Iteration: 50 | Loss: 0.216\nEpoch: 17 | Iteration: 100 | Loss: 0.212\nEpoch: 17 | Iteration: 150 | Loss: 0.241\nEpoch: 17 | Iteration: 200 | Loss: 0.233\nEpoch: 17 | Iteration: 250 | Loss: 0.257\nEpoch: 17 | Iteration: 300 | Loss: 0.249\nEpoch: 17 | Iteration: 350 | Loss: 0.237\nEpoch 17: Test accuracy: 0.881\nEpoch: 18 | Iteration: 50 | Loss: 0.220\nEpoch: 18 | Iteration: 100 | Loss: 0.214\nEpoch: 18 | Iteration: 150 | Loss: 0.240\nEpoch: 18 | Iteration: 200 | Loss: 0.222\nEpoch: 18 | Iteration: 250 | Loss: 0.224\nEpoch: 18 | Iteration: 300 | Loss: 0.223\nEpoch: 18 | Iteration: 350 | Loss: 0.234\nEpoch 18: Test accuracy: 0.885\nEpoch: 19 | Iteration: 50 | Loss: 0.204\nEpoch: 19 | Iteration: 100 | Loss: 0.219\nEpoch: 19 | Iteration: 150 | Loss: 0.223\nEpoch: 19 | Iteration: 200 | Loss: 0.212\nEpoch: 19 | Iteration: 250 | Loss: 0.211\nEpoch: 19 | Iteration: 300 | Loss: 0.201\nEpoch: 19 | Iteration: 350 | Loss: 0.216\nEpoch 19: Test accuracy: 0.876\nEpoch: 20 | Iteration: 50 | Loss: 0.176\nEpoch: 20 | Iteration: 100 | Loss: 0.201\nEpoch: 20 | Iteration: 150 | Loss: 0.197\nEpoch: 20 | Iteration: 200 | Loss: 0.193\nEpoch: 20 | Iteration: 250 | Loss: 0.210\nEpoch: 20 | Iteration: 300 | Loss: 0.202\nEpoch: 20 | Iteration: 350 | Loss: 0.235\nEpoch 20: Test accuracy: 0.889\nEpoch: 21 | Iteration: 50 | Loss: 0.176\nEpoch: 21 | Iteration: 100 | Loss: 0.185\nEpoch: 21 | Iteration: 150 | Loss: 0.180\nEpoch: 21 | Iteration: 200 | Loss: 0.185\nEpoch: 21 | Iteration: 250 | Loss: 0.199\nEpoch: 21 | Iteration: 300 | Loss: 0.209\nEpoch: 21 | Iteration: 350 | Loss: 0.176\nEpoch 21: Test accuracy: 0.885\nEpoch: 22 | Iteration: 50 | Loss: 0.160\nEpoch: 22 | Iteration: 100 | Loss: 0.183\nEpoch: 22 | Iteration: 150 | Loss: 0.200\nEpoch: 22 | Iteration: 200 | Loss: 0.179\nEpoch: 22 | Iteration: 250 | Loss: 0.188\nEpoch: 22 | Iteration: 300 | Loss: 0.197\nEpoch: 22 | Iteration: 350 | Loss: 0.195\nEpoch 22: Test accuracy: 0.892\nEpoch: 23 | Iteration: 50 | Loss: 0.158\nEpoch: 23 | Iteration: 100 | Loss: 0.151\nEpoch: 23 | Iteration: 150 | Loss: 0.180\nEpoch: 23 | Iteration: 200 | Loss: 0.176\nEpoch: 23 | Iteration: 250 | Loss: 0.177\nEpoch: 23 | Iteration: 300 | Loss: 0.168\nEpoch: 23 | Iteration: 350 | Loss: 0.194\nEpoch 23: Test accuracy: 0.885\nEpoch: 24 | Iteration: 50 | Loss: 0.153\nEpoch: 24 | Iteration: 100 | Loss: 0.157\nEpoch: 24 | Iteration: 150 | Loss: 0.177\nEpoch: 24 | Iteration: 200 | Loss: 0.176\nEpoch: 24 | Iteration: 250 | Loss: 0.172\nEpoch: 24 | Iteration: 300 | Loss: 0.173\nEpoch: 24 | Iteration: 350 | Loss: 0.178\nEpoch 24: Test accuracy: 0.893\nEpoch: 25 | Iteration: 50 | Loss: 0.135\nEpoch: 25 | Iteration: 100 | Loss: 0.138\nEpoch: 25 | Iteration: 150 | Loss: 0.163\nEpoch: 25 | Iteration: 200 | Loss: 0.154\nEpoch: 25 | Iteration: 250 | Loss: 0.163\nEpoch: 25 | Iteration: 300 | Loss: 0.164\nEpoch: 25 | Iteration: 350 | Loss: 0.169\nEpoch 25: Test accuracy: 0.898\nEpoch: 26 | Iteration: 50 | Loss: 0.133\nEpoch: 26 | Iteration: 100 | Loss: 0.142\nEpoch: 26 | Iteration: 150 | Loss: 0.147\nEpoch: 26 | Iteration: 200 | Loss: 0.154\nEpoch: 26 | Iteration: 250 | Loss: 0.156\nEpoch: 26 | Iteration: 300 | Loss: 0.148\nEpoch: 26 | Iteration: 350 | Loss: 0.162\nEpoch 26: Test accuracy: 0.891\nEpoch: 27 | Iteration: 50 | Loss: 0.141\nEpoch: 27 | Iteration: 100 | Loss: 0.139\nEpoch: 27 | Iteration: 150 | Loss: 0.143\nEpoch: 27 | Iteration: 200 | Loss: 0.135\nEpoch: 27 | Iteration: 250 | Loss: 0.137\nEpoch: 27 | Iteration: 300 | Loss: 0.135\nEpoch: 27 | Iteration: 350 | Loss: 0.158\nEpoch 27: Test accuracy: 0.893\nEpoch: 28 | Iteration: 50 | Loss: 0.123\nEpoch: 28 | Iteration: 100 | Loss: 0.122\nEpoch: 28 | Iteration: 150 | Loss: 0.131\nEpoch: 28 | Iteration: 200 | Loss: 0.131\nEpoch: 28 | Iteration: 250 | Loss: 0.137\nEpoch: 28 | Iteration: 300 | Loss: 0.125\nEpoch: 28 | Iteration: 350 | Loss: 0.136\nEpoch 28: Test accuracy: 0.892\nEpoch: 29 | Iteration: 50 | Loss: 0.117\nEpoch: 29 | Iteration: 100 | Loss: 0.115\nEpoch: 29 | Iteration: 150 | Loss: 0.118\nEpoch: 29 | Iteration: 200 | Loss: 0.127\nEpoch: 29 | Iteration: 250 | Loss: 0.133\nEpoch: 29 | Iteration: 300 | Loss: 0.149\nEpoch: 29 | Iteration: 350 | Loss: 0.139\nEpoch 29: Test accuracy: 0.900\nEpoch: 30 | Iteration: 50 | Loss: 0.103\nEpoch: 30 | Iteration: 100 | Loss: 0.119\nEpoch: 30 | Iteration: 150 | Loss: 0.119\nEpoch: 30 | Iteration: 200 | Loss: 0.125\nEpoch: 30 | Iteration: 250 | Loss: 0.130\nEpoch: 30 | Iteration: 300 | Loss: 0.107\nEpoch: 30 | Iteration: 350 | Loss: 0.122\nEpoch 30: Test accuracy: 0.896\nEpoch: 31 | Iteration: 50 | Loss: 0.109\nEpoch: 31 | Iteration: 100 | Loss: 0.114\nEpoch: 31 | Iteration: 150 | Loss: 0.124\nEpoch: 31 | Iteration: 200 | Loss: 0.112\nEpoch: 31 | Iteration: 250 | Loss: 0.120\nEpoch: 31 | Iteration: 300 | Loss: 0.133\nEpoch: 31 | Iteration: 350 | Loss: 0.118\nEpoch 31: Test accuracy: 0.895\nEpoch: 32 | Iteration: 50 | Loss: 0.106\nEpoch: 32 | Iteration: 100 | Loss: 0.107\nEpoch: 32 | Iteration: 150 | Loss: 0.122\nEpoch: 32 | Iteration: 200 | Loss: 0.122\nEpoch: 32 | Iteration: 250 | Loss: 0.106\nEpoch: 32 | Iteration: 300 | Loss: 0.125\nEpoch: 32 | Iteration: 350 | Loss: 0.114\nEpoch 32: Test accuracy: 0.893\nEpoch: 33 | Iteration: 50 | Loss: 0.093\nEpoch: 33 | Iteration: 100 | Loss: 0.095\nEpoch: 33 | Iteration: 150 | Loss: 0.103\nEpoch: 33 | Iteration: 200 | Loss: 0.101\nEpoch: 33 | Iteration: 250 | Loss: 0.108\nEpoch: 33 | Iteration: 300 | Loss: 0.106\nEpoch: 33 | Iteration: 350 | Loss: 0.112\nEpoch 33: Test accuracy: 0.900\nEpoch: 34 | Iteration: 50 | Loss: 0.094\nEpoch: 34 | Iteration: 100 | Loss: 0.094\nEpoch: 34 | Iteration: 150 | Loss: 0.089\nEpoch: 34 | Iteration: 200 | Loss: 0.097\nEpoch: 34 | Iteration: 250 | Loss: 0.116\nEpoch: 34 | Iteration: 300 | Loss: 0.131\nEpoch: 34 | Iteration: 350 | Loss: 0.110\nEpoch 34: Test accuracy: 0.906\nEpoch: 35 | Iteration: 50 | Loss: 0.082\nEpoch: 35 | Iteration: 100 | Loss: 0.082\nEpoch: 35 | Iteration: 150 | Loss: 0.095\nEpoch: 35 | Iteration: 200 | Loss: 0.118\nEpoch: 35 | Iteration: 250 | Loss: 0.106\nEpoch: 35 | Iteration: 300 | Loss: 0.113\nEpoch: 35 | Iteration: 350 | Loss: 0.092\nEpoch 35: Test accuracy: 0.898\nEpoch: 36 | Iteration: 50 | Loss: 0.090\nEpoch: 36 | Iteration: 100 | Loss: 0.099\nEpoch: 36 | Iteration: 150 | Loss: 0.102\nEpoch: 36 | Iteration: 200 | Loss: 0.098\nEpoch: 36 | Iteration: 250 | Loss: 0.092\nEpoch: 36 | Iteration: 300 | Loss: 0.102\nEpoch: 36 | Iteration: 350 | Loss: 0.102\nEpoch 36: Test accuracy: 0.901\nEpoch: 37 | Iteration: 50 | Loss: 0.088\nEpoch: 37 | Iteration: 100 | Loss: 0.088\nEpoch: 37 | Iteration: 150 | Loss: 0.083\nEpoch: 37 | Iteration: 200 | Loss: 0.080\nEpoch: 37 | Iteration: 250 | Loss: 0.098\nEpoch: 37 | Iteration: 300 | Loss: 0.096\nEpoch: 37 | Iteration: 350 | Loss: 0.109\nEpoch 37: Test accuracy: 0.902\nEpoch: 38 | Iteration: 50 | Loss: 0.087\nEpoch: 38 | Iteration: 100 | Loss: 0.082\nEpoch: 38 | Iteration: 150 | Loss: 0.076\nEpoch: 38 | Iteration: 200 | Loss: 0.107\nEpoch: 38 | Iteration: 250 | Loss: 0.099\nEpoch: 38 | Iteration: 300 | Loss: 0.088\nEpoch: 38 | Iteration: 350 | Loss: 0.092\nEpoch 38: Test accuracy: 0.910\nEpoch: 39 | Iteration: 50 | Loss: 0.085\nEpoch: 39 | Iteration: 100 | Loss: 0.071\nEpoch: 39 | Iteration: 150 | Loss: 0.086\nEpoch: 39 | Iteration: 200 | Loss: 0.087\nEpoch: 39 | Iteration: 250 | Loss: 0.089\nEpoch: 39 | Iteration: 300 | Loss: 0.094\nEpoch: 39 | Iteration: 350 | Loss: 0.104\nEpoch 39: Test accuracy: 0.899\nEpoch: 40 | Iteration: 50 | Loss: 0.082\nEpoch: 40 | Iteration: 100 | Loss: 0.079\nEpoch: 40 | Iteration: 150 | Loss: 0.087\nEpoch: 40 | Iteration: 200 | Loss: 0.072\nEpoch: 40 | Iteration: 250 | Loss: 0.080\nEpoch: 40 | Iteration: 300 | Loss: 0.081\nEpoch: 40 | Iteration: 350 | Loss: 0.080\nEpoch 40: Test accuracy: 0.903\nEpoch: 41 | Iteration: 50 | Loss: 0.067\nEpoch: 41 | Iteration: 100 | Loss: 0.072\nEpoch: 41 | Iteration: 150 | Loss: 0.083\nEpoch: 41 | Iteration: 200 | Loss: 0.083\nEpoch: 41 | Iteration: 250 | Loss: 0.096\nEpoch: 41 | Iteration: 300 | Loss: 0.088\nEpoch: 41 | Iteration: 350 | Loss: 0.073\nEpoch 41: Test accuracy: 0.904\nEpoch: 42 | Iteration: 50 | Loss: 0.081\nEpoch: 42 | Iteration: 100 | Loss: 0.059\nEpoch: 42 | Iteration: 150 | Loss: 0.068\nEpoch: 42 | Iteration: 200 | Loss: 0.066\nEpoch: 42 | Iteration: 250 | Loss: 0.082\nEpoch: 42 | Iteration: 300 | Loss: 0.076\nEpoch: 42 | Iteration: 350 | Loss: 0.079\nEpoch 42: Test accuracy: 0.905\nEpoch: 43 | Iteration: 50 | Loss: 0.060\nEpoch: 43 | Iteration: 100 | Loss: 0.075\nEpoch: 43 | Iteration: 150 | Loss: 0.063\nEpoch: 43 | Iteration: 200 | Loss: 0.076\nEpoch: 43 | Iteration: 250 | Loss: 0.078\nEpoch: 43 | Iteration: 300 | Loss: 0.094\nEpoch: 43 | Iteration: 350 | Loss: 0.086\nEpoch 43: Test accuracy: 0.904\nEpoch: 44 | Iteration: 50 | Loss: 0.065\nEpoch: 44 | Iteration: 100 | Loss: 0.071\nEpoch: 44 | Iteration: 150 | Loss: 0.077\nEpoch: 44 | Iteration: 200 | Loss: 0.074\nEpoch: 44 | Iteration: 250 | Loss: 0.075\nEpoch: 44 | Iteration: 300 | Loss: 0.070\nEpoch: 44 | Iteration: 350 | Loss: 0.074\nEpoch 44: Test accuracy: 0.908\nEpoch: 45 | Iteration: 50 | Loss: 0.071\nEpoch: 45 | Iteration: 100 | Loss: 0.063\nEpoch: 45 | Iteration: 150 | Loss: 0.071\nEpoch: 45 | Iteration: 200 | Loss: 0.073\nEpoch: 45 | Iteration: 250 | Loss: 0.067\nEpoch: 45 | Iteration: 300 | Loss: 0.068\nEpoch: 45 | Iteration: 350 | Loss: 0.069\nEpoch 45: Test accuracy: 0.898\nEpoch: 46 | Iteration: 50 | Loss: 0.071\nEpoch: 46 | Iteration: 100 | Loss: 0.066\nEpoch: 46 | Iteration: 150 | Loss: 0.071\nEpoch: 46 | Iteration: 200 | Loss: 0.063\nEpoch: 46 | Iteration: 250 | Loss: 0.084\nEpoch: 46 | Iteration: 300 | Loss: 0.067\nEpoch: 46 | Iteration: 350 | Loss: 0.072\nEpoch 46: Test accuracy: 0.904\nEpoch: 47 | Iteration: 50 | Loss: 0.053\nEpoch: 47 | Iteration: 100 | Loss: 0.055\nEpoch: 47 | Iteration: 150 | Loss: 0.064\nEpoch: 47 | Iteration: 200 | Loss: 0.059\nEpoch: 47 | Iteration: 250 | Loss: 0.072\nEpoch: 47 | Iteration: 300 | Loss: 0.065\nEpoch: 47 | Iteration: 350 | Loss: 0.066\nEpoch 47: Test accuracy: 0.900\nEpoch: 48 | Iteration: 50 | Loss: 0.051\nEpoch: 48 | Iteration: 100 | Loss: 0.057\nEpoch: 48 | Iteration: 150 | Loss: 0.060\nEpoch: 48 | Iteration: 200 | Loss: 0.064\nEpoch: 48 | Iteration: 250 | Loss: 0.057\nEpoch: 48 | Iteration: 300 | Loss: 0.067\nEpoch: 48 | Iteration: 350 | Loss: 0.082\nEpoch 48: Test accuracy: 0.912\nEpoch: 49 | Iteration: 50 | Loss: 0.063\nEpoch: 49 | Iteration: 100 | Loss: 0.066\nEpoch: 49 | Iteration: 150 | Loss: 0.058\nEpoch: 49 | Iteration: 200 | Loss: 0.059\nEpoch: 49 | Iteration: 250 | Loss: 0.063\nEpoch: 49 | Iteration: 300 | Loss: 0.075\nEpoch: 49 | Iteration: 350 | Loss: 0.065\nEpoch 49: Test accuracy: 0.901\nEpoch: 50 | Iteration: 50 | Loss: 0.054\nEpoch: 50 | Iteration: 100 | Loss: 0.050\nEpoch: 50 | Iteration: 150 | Loss: 0.054\nEpoch: 50 | Iteration: 200 | Loss: 0.057\nEpoch: 50 | Iteration: 250 | Loss: 0.062\nEpoch: 50 | Iteration: 300 | Loss: 0.067\nEpoch: 50 | Iteration: 350 | Loss: 0.056\nEpoch 50: Test accuracy: 0.910\nModel checkpoint saved to model_checkpoint.pth\n","output_type":"stream"}],"execution_count":7},{"id":"load_test_data_cell","cell_type":"code","source":"# Template code for reading the test file\ndef load_cifar_batch(file):\n    with open(file, 'rb') as fo:\n        batch = pickle.load(fo, encoding='bytes')\n    return batch\n\n# Load the test batch (update the file path if necessary)\ncifar10_batch = load_cifar_batch('/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl')\n\n# Extract images; the test data is in (N x W x H x C) format\nimages = cifar10_batch[b'data']\nprint(f\"Loaded test batch with {images.shape[0]} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:19:55.079334Z","iopub.execute_input":"2025-03-02T11:19:55.079635Z","iopub.status.idle":"2025-03-02T11:19:55.129398Z","shell.execute_reply.started":"2025-03-02T11:19:55.079606Z","shell.execute_reply":"2025-03-02T11:19:55.128608Z"}},"outputs":[{"name":"stdout","text":"Loaded test batch with 10000 images\n","output_type":"stream"}],"execution_count":8},{"id":"create_test_dataset_cell","cell_type":"code","source":"# Create a dataset from the images array\nclass TestDatasetFromArray(Dataset):\n    def __init__(self, images, transform=None):\n        self.images = images\n        if transform is None:\n            # Default transform: convert numpy array to PIL Image, then to tensor, then normalize\n            self.transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n            ])\n        else:\n            self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = self.images[idx]\n        if isinstance(img, np.ndarray):\n            img = img.astype('uint8')\n        if self.transform:\n            img = self.transform(img)\n        return img\n\n# Create the test dataset and dataloader\ntest_dataset = TestDatasetFromArray(images)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:19:55.131201Z","iopub.execute_input":"2025-03-02T11:19:55.131469Z","iopub.status.idle":"2025-03-02T11:19:55.137660Z","shell.execute_reply.started":"2025-03-02T11:19:55.131447Z","shell.execute_reply":"2025-03-02T11:19:55.136853Z"}},"outputs":[],"execution_count":9},{"id":"inference_mode_cell","cell_type":"code","source":"# Inference Mode\n# This cell loads a saved model checkpoint (if available) and runs inference on the test dataset.\n# Predictions are then saved to a CSV file.\n\nif os.path.exists(\"model_checkpoint.pth\"):\n    model.load_state_dict(torch.load(\"model_checkpoint.pth\", map_location=device))\n    print(\"Loaded model checkpoint from model_checkpoint.pth\")\nelse:\n    print(\"No checkpoint found. Running inference with untrained model.\")\n\nmodel.eval()\npredictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        outputs = model(batch)\n        _, preds = torch.max(outputs, 1)\n        predictions.extend(preds.cpu().numpy().tolist())\n\n# Save predictions to CSV using pandas\ndf = pd.DataFrame({\"Id\": range(len(predictions)), \"Prediction\": predictions})\ndf.to_csv(\"submission.csv\", index=False)\nprint(\"Predictions saved to submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T11:28:45.488150Z","iopub.execute_input":"2025-03-02T11:28:45.488569Z","iopub.status.idle":"2025-03-02T11:28:48.562936Z","shell.execute_reply.started":"2025-03-02T11:28:45.488528Z","shell.execute_reply":"2025-03-02T11:28:48.561988Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-11-e03c20ea7edc>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"model_checkpoint.pth\", map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Loaded model checkpoint from model_checkpoint.pth\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":11},{"id":"22c685f2-8590-4b7b-8111-4f1c1e089fe6","cell_type":"markdown","source":"## Instructions\n\n1. **Training Section:** Run the training cell to train the model on CIFAR-10. The model checkpoint will be saved as `model_checkpoint.pth`.\n2. **Inference Section:** Ensure the test pickle file is available at `/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl`. Then run the inference cell to generate predictions, which will be saved to `cifar_test_predictions.csv`.\n\nFeel free to adjust hyperparameters (like epochs) as needed.","metadata":{}}]}