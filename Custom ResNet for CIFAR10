{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":93057,"databundleVersionId":11145869,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2fa738dd-1c6c-4248-b7c6-5c2119398dbe","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:34:59.887481Z","iopub.execute_input":"2025-03-09T22:34:59.887828Z","iopub.status.idle":"2025-03-09T22:35:00.849486Z","shell.execute_reply.started":"2025-03-09T22:34:59.887798Z","shell.execute_reply":"2025-03-09T22:35:00.848365Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_1\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_2\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/batches.meta\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/test_batch\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_3\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_5\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_4\n/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/readme.html\n","output_type":"stream"}],"execution_count":1},{"id":"3a6dc75f-80e2-47c5-886b-f2b80467eb24","cell_type":"markdown","source":"# Custom ResNet for CIFAR-10 on CUDA\n\nThis notebook implements a custom ResNet model for CIFAR-10 using PyTorch. It includes two major sections:\n\n1. **Training Section:** Loads the CIFAR-10 training and test sets via torchvision, trains the model, and saves a checkpoint.\n\n2. **Inference Section:** Loads a test batch from a pickle file (with no labels), runs inference using the trained model (if available), and saves the predictions to a CSV file.\n\nMake sure the test pickle file is located at `/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl` for inference.","metadata":{}},{"id":"imports","cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport pandas as pd\n\n\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:35:00.850823Z","iopub.execute_input":"2025-03-09T22:35:00.851317Z","iopub.status.idle":"2025-03-09T22:35:08.375566Z","shell.execute_reply.started":"2025-03-09T22:35:00.851281Z","shell.execute_reply":"2025-03-09T22:35:08.374649Z"}},"outputs":[],"execution_count":2},{"id":"model_definition","cell_type":"code","source":"# Model Definitions\n\n# class SelfAttention(nn.Module):\n#     def __init__(self, in_channels):\n#         super(SelfAttention, self).__init__()\n#         self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n#         self.key_conv   = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n#         self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n#         self.gamma = nn.Parameter(torch.zeros(1))\n    \n#     def forward(self, x):\n#         B, C, H, W = x.size()\n#         N = H * W\n#         # Compute query, key, and value maps\n#         query = self.query_conv(x).view(B, -1, N)      # (B, C', N)\n#         key   = self.key_conv(x).view(B, -1, N)          # (B, C', N)\n#         value = self.value_conv(x).view(B, -1, N)        # (B, C, N)\n#         # Compute attention scores\n#         attention = torch.bmm(query.transpose(1, 2), key)  # (B, N, N)\n#         attention = F.softmax(attention, dim=-1)\n#         # Compute weighted sum of values\n#         out = torch.bmm(value, attention.transpose(1, 2))  # (B, C, N)\n#         out = out.view(B, C, H, W)\n#         # Scale attention output and add residual connection\n#         out = self.gamma * out + x\n#         return out\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = self.relu(out)\n        return out\n\n\nclass CustomResNet(nn.Module):\n    def __init__(self, blocks_per_stage=[4, 5, 5, 4], base_channels=32, num_classes=10):\n        \"\"\"\n        Args:\n            blocks_per_stage (list): Four integers specifying the number of residual blocks per stage.\n            base_channels (int): Number of channels for the initial convolution.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(CustomResNet, self).__init__()\n        self.in_channels = base_channels\n        # Initial convolution for CIFAR-10 (3-channel, 32x32 images)\n        self.conv1 = nn.Conv2d(3, base_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(base_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.layer1 = self._make_layer(base_channels, blocks_per_stage[0], stride=1)\n        self.layer2 = self._make_layer(base_channels * 2, blocks_per_stage[1], stride=2)\n        self.layer3 = self._make_layer(base_channels * 4, blocks_per_stage[2], stride=2)\n        self.layer4 = self._make_layer(base_channels * 8, blocks_per_stage[3], stride=2)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(p=0.5)  # Add dropout with a dropout rate of 50%\n        self.fc = nn.Linear(base_channels * 8, num_classes)\n\n    def _make_layer(self, out_channels, num_blocks, stride):\n        layers = []\n        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n        self.in_channels = out_channels\n        for _ in range(1, num_blocks):\n            layers.append(BasicBlock(out_channels, out_channels, stride=1))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.dropout(out)  # Apply dropout before classification\n        out = self.fc(out)\n        return out\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:35:08.377546Z","iopub.execute_input":"2025-03-09T22:35:08.378058Z","iopub.status.idle":"2025-03-09T22:35:08.389335Z","shell.execute_reply.started":"2025-03-09T22:35:08.378023Z","shell.execute_reply":"2025-03-09T22:35:08.388456Z"}},"outputs":[],"execution_count":3},{"id":"data_loading_function","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader, random_split, TensorDataset\nfrom torch.optim.lr_scheduler import StepLR\nfrom PIL import Image\nimport torch.optim.lr_scheduler as lr_scheduler\n\n# Data Loading Function for CIFAR-10 using torchvision\ndef get_data(batch_size=128):\n    transform_train = transforms.Compose([\n        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.RandomRotation(15),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n        \n    train_dataset = torchvision.datasets.CIFAR10(root='/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python', train=True,\n                                                 download=False, transform=transform_train)\n    test_dataset = torchvision.datasets.CIFAR10(root='/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python', train=False,\n                                                download=False, transform=transform_test)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    return train_loader, test_loader\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:35:08.390861Z","iopub.execute_input":"2025-03-09T22:35:08.391177Z","iopub.status.idle":"2025-03-09T22:35:08.417472Z","shell.execute_reply.started":"2025-03-09T22:35:08.391142Z","shell.execute_reply":"2025-03-09T22:35:08.416552Z"}},"outputs":[],"execution_count":4},{"id":"017b47d0-eb2c-4cfc-bd68-eb61d94a2c5b","cell_type":"code","source":"def mixup_data(x, y, alpha=0.2):\n    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:35:08.418354Z","iopub.execute_input":"2025-03-09T22:35:08.418708Z","iopub.status.idle":"2025-03-09T22:35:08.438036Z","shell.execute_reply.started":"2025-03-09T22:35:08.418675Z","shell.execute_reply":"2025-03-09T22:35:08.437379Z"}},"outputs":[],"execution_count":5},{"id":"de810c37-71c5-434c-be54-264e1895c9c7","cell_type":"code","source":"class LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        self.smoothing = smoothing\n\n    def forward(self, pred, target):\n        # pred: (batch, num_classes)\n        num_classes = pred.size(1)\n        log_prob = torch.log_softmax(pred, dim=1)\n        with torch.no_grad():\n            # Create smoothed labels\n            true_dist = torch.zeros_like(log_prob)\n            true_dist.fill_(self.smoothing / (num_classes - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), 1 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_prob, dim=1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:35:08.438756Z","iopub.execute_input":"2025-03-09T22:35:08.438963Z","iopub.status.idle":"2025-03-09T22:35:08.461508Z","shell.execute_reply.started":"2025-03-09T22:35:08.438946Z","shell.execute_reply":"2025-03-09T22:35:08.460718Z"}},"outputs":[],"execution_count":6},{"id":"training_testing_functions","cell_type":"code","source":"# Training and Testing Functions\ndef train(model, train_loader, optimizer, criterion, device, epoch, print_interval=50):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, targets) in enumerate(train_loader, 1):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if i % print_interval == 0:\n            print(f\"Epoch: {epoch} | Iteration: {i} | Loss: {running_loss / print_interval:.3f}\")\n            running_loss = 0.0\n\ndef test(model, val_loader, device, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n    accuracy = correct / total\n    print(f\"Epoch {epoch}: Test accuracy: {accuracy:.4f}\")\n    return accuracy\n\ndef train_with_scheduler(model, train_loader, val_loader, optimizer, criterion, device, epochs, print_interval=50):\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for i, (inputs, targets) in enumerate(train_loader, 1):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            if i % print_interval == 0:\n                print(f\"Epoch: {epoch} | Iteration: {i} | Loss: {running_loss / print_interval:.3f}\")\n                running_loss = 0.0\n        scheduler.step()  # Update learning rate after each epoch\n        test_acc = test(model, val_loader, device, epoch)\n        print(f\"Epoch {epoch} Test Accuracy: {test_acc:.3f}\")\n\ndef train_with_mixup(model, train_loader, optimizer, criterion, device, epochs, print_interval=50, alpha=0.2):\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for i, (inputs, targets) in enumerate(train_loader, 1):\n            inputs, targets = inputs.to(device), targets.to(device)\n            # Apply mixup augmentation\n            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=alpha)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            if i % print_interval == 0:\n                print(f\"Epoch: {epoch} | Iteration: {i} | Loss: {running_loss / print_interval:.3f}\")\n                running_loss = 0.0\n        test_acc = test(model, *get_data(batch_size=128)[1:], device, epoch)  # use your test() function\n        print(f\"Epoch {epoch}: Test Accuracy = {test_acc:.4f}\")\n\ndef train_mixup_onecycle(model, train_loader, val_loader, optimizer, criterion, device, epochs, scheduler, print_interval=50, alpha=0.2):\n    \"\"\"\n    Trains the model using mixup augmentation and OneCycleLR scheduling.\n    \n    Parameters:\n      - model: the neural network model.\n      - train_loader: DataLoader for the training set.\n      - val_loader: DataLoader for the validation/test set.\n      - optimizer: optimizer (e.g., SGD with momentum).\n      - criterion: loss function (e.g., CrossEntropyLoss).\n      - device: device to run training on.\n      - epochs: number of epochs to train.\n      - print_interval: how many iterations before printing loss.\n      - alpha: mixup interpolation coefficient.\n    \"\"\"\n    total_steps = epochs * len(train_loader)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=0.1, total_steps=total_steps,\n        pct_start=0.3, anneal_strategy='cos', div_factor=25.0\n    )\n    \n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for i, (inputs, targets) in enumerate(train_loader, 1):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Apply mixup augmentation\n            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()  # Update LR per batch\n            running_loss += loss.item()\n            if i % print_interval == 0:\n                print(f\"Epoch: {epoch} | Iteration: {i} | Loss: {running_loss / print_interval:.3f}\")\n                running_loss = 0.0\n                \n                \n        test_acc = test(model, val_loader, device, epoch)\n        # print(f\"Epoch {epoch}: Test Accuracy = {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:35:08.462390Z","iopub.execute_input":"2025-03-09T22:35:08.462726Z","iopub.status.idle":"2025-03-09T22:35:08.478760Z","shell.execute_reply.started":"2025-03-09T22:35:08.462693Z","shell.execute_reply":"2025-03-09T22:35:08.477984Z"}},"outputs":[],"execution_count":7},{"id":"device_model_init","cell_type":"code","source":"# Set device: Use Apple MPS if available, otherwise CUDA or CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cuda\")\nprint(\"Using device:\", device)\n\n# Instantiate the model and print the parameter count\n# model = CustomResNet(blocks_per_stage=[3, 4, 4, 3], base_channels=32, num_classes=10).to(device)\n# print(\"Total parameters:\", count_parameters(model))\n\n# Option B: Slight increase in depth\nmodel = CustomResNet(blocks_per_stage=[9, 6, 4, 3], base_channels=32, num_classes=10).to(device)\nprint(\"Total parameters:\", count_parameters(model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:35:08.481130Z","iopub.execute_input":"2025-03-09T22:35:08.481413Z","iopub.status.idle":"2025-03-09T22:35:08.872730Z","shell.execute_reply.started":"2025-03-09T22:35:08.481384Z","shell.execute_reply":"2025-03-09T22:35:08.871898Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTotal parameters: 4994986\n","output_type":"stream"}],"execution_count":8},{"id":"training_mode_cell","cell_type":"code","source":"# Training Mode\n# This cell trains the model on CIFAR-10 and saves a checkpoint.\nimport torch.nn.functional as F\ntrain_loader, val_loader = get_data(batch_size=128)\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n# criterion = nn.CrossEntropyLoss()\n# epochs = 50  # Adjust epochs as needed\n\n# Example optimizer (using SGD with momentum, weight decay, and higher lr)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\ncriterion = nn.CrossEntropyLoss()\nepochs = 150  # You can adjust as needed\n\n# Call the updated training function:\n\n\n# for epoch in range(1, epochs + 1):\n    # train(model, train_loader, optimizer, criterion, device, epoch)\n    # test(model, val_loader, device, epoch)\n#train_with_scheduler(model, train_loader, val_loader, optimizer, criterion, device, epochs)\ntrain_mixup_onecycle(model, train_loader, val_loader, optimizer, criterion, device, epochs, scheduler)\n\n# Save the trained model checkpoint\ntorch.save(model.state_dict(), \"model_checkpoint.pth\")\nprint(\"Model checkpoint saved to model_checkpoint.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T22:35:08.873792Z","iopub.execute_input":"2025-03-09T22:35:08.874010Z","iopub.status.idle":"2025-03-10T00:23:00.737042Z","shell.execute_reply.started":"2025-03-09T22:35:08.873991Z","shell.execute_reply":"2025-03-10T00:23:00.735951Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1 | Iteration: 50 | Loss: 2.442\nEpoch: 1 | Iteration: 100 | Loss: 2.261\nEpoch: 1 | Iteration: 150 | Loss: 2.168\nEpoch: 1 | Iteration: 200 | Loss: 2.114\nEpoch: 1 | Iteration: 250 | Loss: 2.085\nEpoch: 1 | Iteration: 300 | Loss: 2.036\nEpoch: 1 | Iteration: 350 | Loss: 2.042\nEpoch 1: Test accuracy: 0.3375\nEpoch: 2 | Iteration: 50 | Loss: 2.020\nEpoch: 2 | Iteration: 100 | Loss: 1.958\nEpoch: 2 | Iteration: 150 | Loss: 1.957\nEpoch: 2 | Iteration: 200 | Loss: 1.941\nEpoch: 2 | Iteration: 250 | Loss: 1.967\nEpoch: 2 | Iteration: 300 | Loss: 1.870\nEpoch: 2 | Iteration: 350 | Loss: 1.866\nEpoch 2: Test accuracy: 0.4813\nEpoch: 3 | Iteration: 50 | Loss: 1.831\nEpoch: 3 | Iteration: 100 | Loss: 1.871\nEpoch: 3 | Iteration: 150 | Loss: 1.778\nEpoch: 3 | Iteration: 200 | Loss: 1.794\nEpoch: 3 | Iteration: 250 | Loss: 1.721\nEpoch: 3 | Iteration: 300 | Loss: 1.766\nEpoch: 3 | Iteration: 350 | Loss: 1.740\nEpoch 3: Test accuracy: 0.5328\nEpoch: 4 | Iteration: 50 | Loss: 1.672\nEpoch: 4 | Iteration: 100 | Loss: 1.696\nEpoch: 4 | Iteration: 150 | Loss: 1.620\nEpoch: 4 | Iteration: 200 | Loss: 1.665\nEpoch: 4 | Iteration: 250 | Loss: 1.630\nEpoch: 4 | Iteration: 300 | Loss: 1.667\nEpoch: 4 | Iteration: 350 | Loss: 1.679\nEpoch 4: Test accuracy: 0.5788\nEpoch: 5 | Iteration: 50 | Loss: 1.596\nEpoch: 5 | Iteration: 100 | Loss: 1.569\nEpoch: 5 | Iteration: 150 | Loss: 1.591\nEpoch: 5 | Iteration: 200 | Loss: 1.522\nEpoch: 5 | Iteration: 250 | Loss: 1.509\nEpoch: 5 | Iteration: 300 | Loss: 1.503\nEpoch: 5 | Iteration: 350 | Loss: 1.617\nEpoch 5: Test accuracy: 0.6456\nEpoch: 6 | Iteration: 50 | Loss: 1.479\nEpoch: 6 | Iteration: 100 | Loss: 1.541\nEpoch: 6 | Iteration: 150 | Loss: 1.562\nEpoch: 6 | Iteration: 200 | Loss: 1.514\nEpoch: 6 | Iteration: 250 | Loss: 1.454\nEpoch: 6 | Iteration: 300 | Loss: 1.515\nEpoch: 6 | Iteration: 350 | Loss: 1.393\nEpoch 6: Test accuracy: 0.6678\nEpoch: 7 | Iteration: 50 | Loss: 1.421\nEpoch: 7 | Iteration: 100 | Loss: 1.460\nEpoch: 7 | Iteration: 150 | Loss: 1.395\nEpoch: 7 | Iteration: 200 | Loss: 1.442\nEpoch: 7 | Iteration: 250 | Loss: 1.499\nEpoch: 7 | Iteration: 300 | Loss: 1.424\nEpoch: 7 | Iteration: 350 | Loss: 1.403\nEpoch 7: Test accuracy: 0.7145\nEpoch: 8 | Iteration: 50 | Loss: 1.349\nEpoch: 8 | Iteration: 100 | Loss: 1.392\nEpoch: 8 | Iteration: 150 | Loss: 1.372\nEpoch: 8 | Iteration: 200 | Loss: 1.453\nEpoch: 8 | Iteration: 250 | Loss: 1.375\nEpoch: 8 | Iteration: 300 | Loss: 1.310\nEpoch: 8 | Iteration: 350 | Loss: 1.434\nEpoch 8: Test accuracy: 0.6561\nEpoch: 9 | Iteration: 50 | Loss: 1.353\nEpoch: 9 | Iteration: 100 | Loss: 1.347\nEpoch: 9 | Iteration: 150 | Loss: 1.294\nEpoch: 9 | Iteration: 200 | Loss: 1.324\nEpoch: 9 | Iteration: 250 | Loss: 1.372\nEpoch: 9 | Iteration: 300 | Loss: 1.332\nEpoch: 9 | Iteration: 350 | Loss: 1.336\nEpoch 9: Test accuracy: 0.7427\nEpoch: 10 | Iteration: 50 | Loss: 1.379\nEpoch: 10 | Iteration: 100 | Loss: 1.340\nEpoch: 10 | Iteration: 150 | Loss: 1.357\nEpoch: 10 | Iteration: 200 | Loss: 1.305\nEpoch: 10 | Iteration: 250 | Loss: 1.386\nEpoch: 10 | Iteration: 300 | Loss: 1.355\nEpoch: 10 | Iteration: 350 | Loss: 1.323\nEpoch 10: Test accuracy: 0.7450\nEpoch: 11 | Iteration: 50 | Loss: 1.430\nEpoch: 11 | Iteration: 100 | Loss: 1.362\nEpoch: 11 | Iteration: 150 | Loss: 1.397\nEpoch: 11 | Iteration: 200 | Loss: 1.370\nEpoch: 11 | Iteration: 250 | Loss: 1.369\nEpoch: 11 | Iteration: 300 | Loss: 1.409\nEpoch: 11 | Iteration: 350 | Loss: 1.302\nEpoch 11: Test accuracy: 0.7020\nEpoch: 12 | Iteration: 50 | Loss: 1.356\nEpoch: 12 | Iteration: 100 | Loss: 1.340\nEpoch: 12 | Iteration: 150 | Loss: 1.330\nEpoch: 12 | Iteration: 200 | Loss: 1.232\nEpoch: 12 | Iteration: 250 | Loss: 1.306\nEpoch: 12 | Iteration: 300 | Loss: 1.303\nEpoch: 12 | Iteration: 350 | Loss: 1.326\nEpoch 12: Test accuracy: 0.7332\nEpoch: 13 | Iteration: 50 | Loss: 1.272\nEpoch: 13 | Iteration: 100 | Loss: 1.398\nEpoch: 13 | Iteration: 150 | Loss: 1.332\nEpoch: 13 | Iteration: 200 | Loss: 1.300\nEpoch: 13 | Iteration: 250 | Loss: 1.272\nEpoch: 13 | Iteration: 300 | Loss: 1.227\nEpoch: 13 | Iteration: 350 | Loss: 1.284\nEpoch 13: Test accuracy: 0.7663\nEpoch: 14 | Iteration: 50 | Loss: 1.237\nEpoch: 14 | Iteration: 100 | Loss: 1.256\nEpoch: 14 | Iteration: 150 | Loss: 1.263\nEpoch: 14 | Iteration: 200 | Loss: 1.183\nEpoch: 14 | Iteration: 250 | Loss: 1.110\nEpoch: 14 | Iteration: 300 | Loss: 1.222\nEpoch: 14 | Iteration: 350 | Loss: 1.165\nEpoch 14: Test accuracy: 0.7543\nEpoch: 15 | Iteration: 50 | Loss: 1.302\nEpoch: 15 | Iteration: 100 | Loss: 1.232\nEpoch: 15 | Iteration: 150 | Loss: 1.196\nEpoch: 15 | Iteration: 200 | Loss: 1.173\nEpoch: 15 | Iteration: 250 | Loss: 1.231\nEpoch: 15 | Iteration: 300 | Loss: 1.144\nEpoch: 15 | Iteration: 350 | Loss: 1.275\nEpoch 15: Test accuracy: 0.7857\nEpoch: 16 | Iteration: 50 | Loss: 1.240\nEpoch: 16 | Iteration: 100 | Loss: 1.187\nEpoch: 16 | Iteration: 150 | Loss: 1.134\nEpoch: 16 | Iteration: 200 | Loss: 1.269\nEpoch: 16 | Iteration: 250 | Loss: 1.169\nEpoch: 16 | Iteration: 300 | Loss: 1.200\nEpoch: 16 | Iteration: 350 | Loss: 1.191\nEpoch 16: Test accuracy: 0.8036\nEpoch: 17 | Iteration: 50 | Loss: 1.210\nEpoch: 17 | Iteration: 100 | Loss: 1.035\nEpoch: 17 | Iteration: 150 | Loss: 1.173\nEpoch: 17 | Iteration: 200 | Loss: 1.120\nEpoch: 17 | Iteration: 250 | Loss: 1.172\nEpoch: 17 | Iteration: 300 | Loss: 1.132\nEpoch: 17 | Iteration: 350 | Loss: 1.189\nEpoch 17: Test accuracy: 0.7867\nEpoch: 18 | Iteration: 50 | Loss: 1.202\nEpoch: 18 | Iteration: 100 | Loss: 1.188\nEpoch: 18 | Iteration: 150 | Loss: 1.209\nEpoch: 18 | Iteration: 200 | Loss: 1.169\nEpoch: 18 | Iteration: 250 | Loss: 1.178\nEpoch: 18 | Iteration: 300 | Loss: 1.117\nEpoch: 18 | Iteration: 350 | Loss: 1.117\nEpoch 18: Test accuracy: 0.8083\nEpoch: 19 | Iteration: 50 | Loss: 1.078\nEpoch: 19 | Iteration: 100 | Loss: 1.232\nEpoch: 19 | Iteration: 150 | Loss: 1.202\nEpoch: 19 | Iteration: 200 | Loss: 1.159\nEpoch: 19 | Iteration: 250 | Loss: 1.187\nEpoch: 19 | Iteration: 300 | Loss: 1.156\nEpoch: 19 | Iteration: 350 | Loss: 1.103\nEpoch 19: Test accuracy: 0.8015\nEpoch: 20 | Iteration: 50 | Loss: 1.098\nEpoch: 20 | Iteration: 100 | Loss: 1.118\nEpoch: 20 | Iteration: 150 | Loss: 1.169\nEpoch: 20 | Iteration: 200 | Loss: 1.035\nEpoch: 20 | Iteration: 250 | Loss: 1.197\nEpoch: 20 | Iteration: 300 | Loss: 1.169\nEpoch: 20 | Iteration: 350 | Loss: 1.175\nEpoch 20: Test accuracy: 0.7843\nEpoch: 21 | Iteration: 50 | Loss: 1.223\nEpoch: 21 | Iteration: 100 | Loss: 1.273\nEpoch: 21 | Iteration: 150 | Loss: 1.194\nEpoch: 21 | Iteration: 200 | Loss: 1.080\nEpoch: 21 | Iteration: 250 | Loss: 1.174\nEpoch: 21 | Iteration: 300 | Loss: 1.101\nEpoch: 21 | Iteration: 350 | Loss: 1.113\nEpoch 21: Test accuracy: 0.8272\nEpoch: 22 | Iteration: 50 | Loss: 1.100\nEpoch: 22 | Iteration: 100 | Loss: 1.096\nEpoch: 22 | Iteration: 150 | Loss: 1.177\nEpoch: 22 | Iteration: 200 | Loss: 1.128\nEpoch: 22 | Iteration: 250 | Loss: 1.080\nEpoch: 22 | Iteration: 300 | Loss: 1.156\nEpoch: 22 | Iteration: 350 | Loss: 1.199\nEpoch 22: Test accuracy: 0.7993\nEpoch: 23 | Iteration: 50 | Loss: 1.145\nEpoch: 23 | Iteration: 100 | Loss: 1.170\nEpoch: 23 | Iteration: 150 | Loss: 1.245\nEpoch: 23 | Iteration: 200 | Loss: 1.108\nEpoch: 23 | Iteration: 250 | Loss: 1.112\nEpoch: 23 | Iteration: 300 | Loss: 1.092\nEpoch: 23 | Iteration: 350 | Loss: 1.198\nEpoch 23: Test accuracy: 0.7737\nEpoch: 24 | Iteration: 50 | Loss: 1.169\nEpoch: 24 | Iteration: 100 | Loss: 1.076\nEpoch: 24 | Iteration: 150 | Loss: 1.134\nEpoch: 24 | Iteration: 200 | Loss: 1.090\nEpoch: 24 | Iteration: 250 | Loss: 1.123\nEpoch: 24 | Iteration: 300 | Loss: 1.188\nEpoch: 24 | Iteration: 350 | Loss: 1.114\nEpoch 24: Test accuracy: 0.7856\nEpoch: 25 | Iteration: 50 | Loss: 1.139\nEpoch: 25 | Iteration: 100 | Loss: 1.216\nEpoch: 25 | Iteration: 150 | Loss: 1.141\nEpoch: 25 | Iteration: 200 | Loss: 1.113\nEpoch: 25 | Iteration: 250 | Loss: 1.192\nEpoch: 25 | Iteration: 300 | Loss: 1.081\nEpoch: 25 | Iteration: 350 | Loss: 1.149\nEpoch 25: Test accuracy: 0.7729\nEpoch: 26 | Iteration: 50 | Loss: 1.133\nEpoch: 26 | Iteration: 100 | Loss: 1.145\nEpoch: 26 | Iteration: 150 | Loss: 1.076\nEpoch: 26 | Iteration: 200 | Loss: 1.018\nEpoch: 26 | Iteration: 250 | Loss: 1.193\nEpoch: 26 | Iteration: 300 | Loss: 1.213\nEpoch: 26 | Iteration: 350 | Loss: 1.129\nEpoch 26: Test accuracy: 0.8097\nEpoch: 27 | Iteration: 50 | Loss: 1.220\nEpoch: 27 | Iteration: 100 | Loss: 1.123\nEpoch: 27 | Iteration: 150 | Loss: 1.132\nEpoch: 27 | Iteration: 200 | Loss: 1.058\nEpoch: 27 | Iteration: 250 | Loss: 1.108\nEpoch: 27 | Iteration: 300 | Loss: 1.098\nEpoch: 27 | Iteration: 350 | Loss: 1.072\nEpoch 27: Test accuracy: 0.8039\nEpoch: 28 | Iteration: 50 | Loss: 1.133\nEpoch: 28 | Iteration: 100 | Loss: 1.086\nEpoch: 28 | Iteration: 150 | Loss: 1.148\nEpoch: 28 | Iteration: 200 | Loss: 1.122\nEpoch: 28 | Iteration: 250 | Loss: 1.112\nEpoch: 28 | Iteration: 300 | Loss: 1.119\nEpoch: 28 | Iteration: 350 | Loss: 1.168\nEpoch 28: Test accuracy: 0.8133\nEpoch: 29 | Iteration: 50 | Loss: 1.140\nEpoch: 29 | Iteration: 100 | Loss: 0.991\nEpoch: 29 | Iteration: 150 | Loss: 1.068\nEpoch: 29 | Iteration: 200 | Loss: 1.064\nEpoch: 29 | Iteration: 250 | Loss: 1.123\nEpoch: 29 | Iteration: 300 | Loss: 1.055\nEpoch: 29 | Iteration: 350 | Loss: 1.085\nEpoch 29: Test accuracy: 0.7809\nEpoch: 30 | Iteration: 50 | Loss: 1.009\nEpoch: 30 | Iteration: 100 | Loss: 1.055\nEpoch: 30 | Iteration: 150 | Loss: 1.031\nEpoch: 30 | Iteration: 200 | Loss: 1.116\nEpoch: 30 | Iteration: 250 | Loss: 1.139\nEpoch: 30 | Iteration: 300 | Loss: 1.030\nEpoch: 30 | Iteration: 350 | Loss: 1.047\nEpoch 30: Test accuracy: 0.8042\nEpoch: 31 | Iteration: 50 | Loss: 1.062\nEpoch: 31 | Iteration: 100 | Loss: 1.139\nEpoch: 31 | Iteration: 150 | Loss: 1.048\nEpoch: 31 | Iteration: 200 | Loss: 1.017\nEpoch: 31 | Iteration: 250 | Loss: 1.094\nEpoch: 31 | Iteration: 300 | Loss: 1.121\nEpoch: 31 | Iteration: 350 | Loss: 1.127\nEpoch 31: Test accuracy: 0.7926\nEpoch: 32 | Iteration: 50 | Loss: 1.160\nEpoch: 32 | Iteration: 100 | Loss: 1.108\nEpoch: 32 | Iteration: 150 | Loss: 1.091\nEpoch: 32 | Iteration: 200 | Loss: 1.061\nEpoch: 32 | Iteration: 250 | Loss: 1.078\nEpoch: 32 | Iteration: 300 | Loss: 1.118\nEpoch: 32 | Iteration: 350 | Loss: 1.180\nEpoch 32: Test accuracy: 0.8096\nEpoch: 33 | Iteration: 50 | Loss: 1.152\nEpoch: 33 | Iteration: 100 | Loss: 1.153\nEpoch: 33 | Iteration: 150 | Loss: 1.162\nEpoch: 33 | Iteration: 200 | Loss: 1.123\nEpoch: 33 | Iteration: 250 | Loss: 0.972\nEpoch: 33 | Iteration: 300 | Loss: 1.103\nEpoch: 33 | Iteration: 350 | Loss: 1.047\nEpoch 33: Test accuracy: 0.8118\nEpoch: 34 | Iteration: 50 | Loss: 1.019\nEpoch: 34 | Iteration: 100 | Loss: 1.130\nEpoch: 34 | Iteration: 150 | Loss: 1.088\nEpoch: 34 | Iteration: 200 | Loss: 1.083\nEpoch: 34 | Iteration: 250 | Loss: 1.104\nEpoch: 34 | Iteration: 300 | Loss: 1.201\nEpoch: 34 | Iteration: 350 | Loss: 1.084\nEpoch 34: Test accuracy: 0.8363\nEpoch: 35 | Iteration: 50 | Loss: 1.095\nEpoch: 35 | Iteration: 100 | Loss: 1.233\nEpoch: 35 | Iteration: 150 | Loss: 1.184\nEpoch: 35 | Iteration: 200 | Loss: 1.040\nEpoch: 35 | Iteration: 250 | Loss: 1.051\nEpoch: 35 | Iteration: 300 | Loss: 1.052\nEpoch: 35 | Iteration: 350 | Loss: 1.163\nEpoch 35: Test accuracy: 0.7878\nEpoch: 36 | Iteration: 50 | Loss: 1.071\nEpoch: 36 | Iteration: 100 | Loss: 1.022\nEpoch: 36 | Iteration: 150 | Loss: 1.233\nEpoch: 36 | Iteration: 200 | Loss: 1.002\nEpoch: 36 | Iteration: 250 | Loss: 1.139\nEpoch: 36 | Iteration: 300 | Loss: 1.042\nEpoch: 36 | Iteration: 350 | Loss: 1.123\nEpoch 36: Test accuracy: 0.8108\nEpoch: 37 | Iteration: 50 | Loss: 1.150\nEpoch: 37 | Iteration: 100 | Loss: 1.093\nEpoch: 37 | Iteration: 150 | Loss: 1.131\nEpoch: 37 | Iteration: 200 | Loss: 1.127\nEpoch: 37 | Iteration: 250 | Loss: 1.170\nEpoch: 37 | Iteration: 300 | Loss: 1.024\nEpoch: 37 | Iteration: 350 | Loss: 1.129\nEpoch 37: Test accuracy: 0.8349\nEpoch: 38 | Iteration: 50 | Loss: 1.081\nEpoch: 38 | Iteration: 100 | Loss: 1.056\nEpoch: 38 | Iteration: 150 | Loss: 0.969\nEpoch: 38 | Iteration: 200 | Loss: 1.134\nEpoch: 38 | Iteration: 250 | Loss: 1.089\nEpoch: 38 | Iteration: 300 | Loss: 1.151\nEpoch: 38 | Iteration: 350 | Loss: 1.189\nEpoch 38: Test accuracy: 0.7839\nEpoch: 39 | Iteration: 50 | Loss: 0.998\nEpoch: 39 | Iteration: 100 | Loss: 1.112\nEpoch: 39 | Iteration: 150 | Loss: 1.049\nEpoch: 39 | Iteration: 200 | Loss: 1.042\nEpoch: 39 | Iteration: 250 | Loss: 1.096\nEpoch: 39 | Iteration: 300 | Loss: 1.068\nEpoch: 39 | Iteration: 350 | Loss: 1.100\nEpoch 39: Test accuracy: 0.8127\nEpoch: 40 | Iteration: 50 | Loss: 1.054\nEpoch: 40 | Iteration: 100 | Loss: 1.057\nEpoch: 40 | Iteration: 150 | Loss: 1.103\nEpoch: 40 | Iteration: 200 | Loss: 1.027\nEpoch: 40 | Iteration: 250 | Loss: 1.032\nEpoch: 40 | Iteration: 300 | Loss: 0.996\nEpoch: 40 | Iteration: 350 | Loss: 1.145\nEpoch 40: Test accuracy: 0.8060\nEpoch: 41 | Iteration: 50 | Loss: 1.130\nEpoch: 41 | Iteration: 100 | Loss: 1.097\nEpoch: 41 | Iteration: 150 | Loss: 1.165\nEpoch: 41 | Iteration: 200 | Loss: 1.068\nEpoch: 41 | Iteration: 250 | Loss: 1.185\nEpoch: 41 | Iteration: 300 | Loss: 1.113\nEpoch: 41 | Iteration: 350 | Loss: 1.094\nEpoch 41: Test accuracy: 0.8036\nEpoch: 42 | Iteration: 50 | Loss: 1.066\nEpoch: 42 | Iteration: 100 | Loss: 1.114\nEpoch: 42 | Iteration: 150 | Loss: 1.027\nEpoch: 42 | Iteration: 200 | Loss: 1.076\nEpoch: 42 | Iteration: 250 | Loss: 1.186\nEpoch: 42 | Iteration: 300 | Loss: 1.041\nEpoch: 42 | Iteration: 350 | Loss: 0.997\nEpoch 42: Test accuracy: 0.8208\nEpoch: 43 | Iteration: 50 | Loss: 1.041\nEpoch: 43 | Iteration: 100 | Loss: 1.020\nEpoch: 43 | Iteration: 150 | Loss: 1.114\nEpoch: 43 | Iteration: 200 | Loss: 1.096\nEpoch: 43 | Iteration: 250 | Loss: 1.018\nEpoch: 43 | Iteration: 300 | Loss: 1.162\nEpoch: 43 | Iteration: 350 | Loss: 1.259\nEpoch 43: Test accuracy: 0.8080\nEpoch: 44 | Iteration: 50 | Loss: 1.095\nEpoch: 44 | Iteration: 100 | Loss: 1.132\nEpoch: 44 | Iteration: 150 | Loss: 1.093\nEpoch: 44 | Iteration: 200 | Loss: 1.099\nEpoch: 44 | Iteration: 250 | Loss: 1.210\nEpoch: 44 | Iteration: 300 | Loss: 1.022\nEpoch: 44 | Iteration: 350 | Loss: 1.002\nEpoch 44: Test accuracy: 0.7689\nEpoch: 45 | Iteration: 50 | Loss: 1.101\nEpoch: 45 | Iteration: 100 | Loss: 1.089\nEpoch: 45 | Iteration: 150 | Loss: 1.051\nEpoch: 45 | Iteration: 200 | Loss: 1.100\nEpoch: 45 | Iteration: 250 | Loss: 1.019\nEpoch: 45 | Iteration: 300 | Loss: 1.062\nEpoch: 45 | Iteration: 350 | Loss: 1.089\nEpoch 45: Test accuracy: 0.8032\nEpoch: 46 | Iteration: 50 | Loss: 1.017\nEpoch: 46 | Iteration: 100 | Loss: 1.109\nEpoch: 46 | Iteration: 150 | Loss: 1.092\nEpoch: 46 | Iteration: 200 | Loss: 1.068\nEpoch: 46 | Iteration: 250 | Loss: 1.099\nEpoch: 46 | Iteration: 300 | Loss: 1.045\nEpoch: 46 | Iteration: 350 | Loss: 1.198\nEpoch 46: Test accuracy: 0.7943\nEpoch: 47 | Iteration: 50 | Loss: 1.137\nEpoch: 47 | Iteration: 100 | Loss: 1.086\nEpoch: 47 | Iteration: 150 | Loss: 1.097\nEpoch: 47 | Iteration: 200 | Loss: 0.982\nEpoch: 47 | Iteration: 250 | Loss: 1.082\nEpoch: 47 | Iteration: 300 | Loss: 0.955\nEpoch: 47 | Iteration: 350 | Loss: 1.057\nEpoch 47: Test accuracy: 0.8386\nEpoch: 48 | Iteration: 50 | Loss: 1.059\nEpoch: 48 | Iteration: 100 | Loss: 1.041\nEpoch: 48 | Iteration: 150 | Loss: 1.024\nEpoch: 48 | Iteration: 200 | Loss: 1.119\nEpoch: 48 | Iteration: 250 | Loss: 1.144\nEpoch: 48 | Iteration: 300 | Loss: 1.035\nEpoch: 48 | Iteration: 350 | Loss: 1.019\nEpoch 48: Test accuracy: 0.7944\nEpoch: 49 | Iteration: 50 | Loss: 1.133\nEpoch: 49 | Iteration: 100 | Loss: 1.083\nEpoch: 49 | Iteration: 150 | Loss: 1.169\nEpoch: 49 | Iteration: 200 | Loss: 1.087\nEpoch: 49 | Iteration: 250 | Loss: 1.020\nEpoch: 49 | Iteration: 300 | Loss: 1.092\nEpoch: 49 | Iteration: 350 | Loss: 1.050\nEpoch 49: Test accuracy: 0.8014\nEpoch: 50 | Iteration: 50 | Loss: 1.058\nEpoch: 50 | Iteration: 100 | Loss: 1.095\nEpoch: 50 | Iteration: 150 | Loss: 1.054\nEpoch: 50 | Iteration: 200 | Loss: 1.058\nEpoch: 50 | Iteration: 250 | Loss: 1.040\nEpoch: 50 | Iteration: 300 | Loss: 1.109\nEpoch: 50 | Iteration: 350 | Loss: 1.124\nEpoch 50: Test accuracy: 0.8318\nEpoch: 51 | Iteration: 50 | Loss: 1.165\nEpoch: 51 | Iteration: 100 | Loss: 1.106\nEpoch: 51 | Iteration: 150 | Loss: 1.062\nEpoch: 51 | Iteration: 200 | Loss: 1.016\nEpoch: 51 | Iteration: 250 | Loss: 1.024\nEpoch: 51 | Iteration: 300 | Loss: 1.073\nEpoch: 51 | Iteration: 350 | Loss: 1.048\nEpoch 51: Test accuracy: 0.8360\nEpoch: 52 | Iteration: 50 | Loss: 1.154\nEpoch: 52 | Iteration: 100 | Loss: 1.085\nEpoch: 52 | Iteration: 150 | Loss: 1.048\nEpoch: 52 | Iteration: 200 | Loss: 1.202\nEpoch: 52 | Iteration: 250 | Loss: 0.977\nEpoch: 52 | Iteration: 300 | Loss: 1.010\nEpoch: 52 | Iteration: 350 | Loss: 0.974\nEpoch 52: Test accuracy: 0.8175\nEpoch: 53 | Iteration: 50 | Loss: 1.082\nEpoch: 53 | Iteration: 100 | Loss: 1.187\nEpoch: 53 | Iteration: 150 | Loss: 1.145\nEpoch: 53 | Iteration: 200 | Loss: 1.062\nEpoch: 53 | Iteration: 250 | Loss: 1.023\nEpoch: 53 | Iteration: 300 | Loss: 1.095\nEpoch: 53 | Iteration: 350 | Loss: 1.085\nEpoch 53: Test accuracy: 0.7838\nEpoch: 54 | Iteration: 50 | Loss: 0.970\nEpoch: 54 | Iteration: 100 | Loss: 1.115\nEpoch: 54 | Iteration: 150 | Loss: 1.028\nEpoch: 54 | Iteration: 200 | Loss: 1.027\nEpoch: 54 | Iteration: 250 | Loss: 1.048\nEpoch: 54 | Iteration: 300 | Loss: 1.035\nEpoch: 54 | Iteration: 350 | Loss: 1.148\nEpoch 54: Test accuracy: 0.8329\nEpoch: 55 | Iteration: 50 | Loss: 1.092\nEpoch: 55 | Iteration: 100 | Loss: 1.051\nEpoch: 55 | Iteration: 150 | Loss: 1.048\nEpoch: 55 | Iteration: 200 | Loss: 0.981\nEpoch: 55 | Iteration: 250 | Loss: 1.143\nEpoch: 55 | Iteration: 300 | Loss: 1.027\nEpoch: 55 | Iteration: 350 | Loss: 0.998\nEpoch 55: Test accuracy: 0.8353\nEpoch: 56 | Iteration: 50 | Loss: 1.093\nEpoch: 56 | Iteration: 100 | Loss: 0.979\nEpoch: 56 | Iteration: 150 | Loss: 1.038\nEpoch: 56 | Iteration: 200 | Loss: 0.985\nEpoch: 56 | Iteration: 250 | Loss: 1.050\nEpoch: 56 | Iteration: 300 | Loss: 1.188\nEpoch: 56 | Iteration: 350 | Loss: 1.022\nEpoch 56: Test accuracy: 0.8404\nEpoch: 57 | Iteration: 50 | Loss: 1.070\nEpoch: 57 | Iteration: 100 | Loss: 1.016\nEpoch: 57 | Iteration: 150 | Loss: 1.102\nEpoch: 57 | Iteration: 200 | Loss: 1.000\nEpoch: 57 | Iteration: 250 | Loss: 1.002\nEpoch: 57 | Iteration: 300 | Loss: 1.095\nEpoch: 57 | Iteration: 350 | Loss: 1.061\nEpoch 57: Test accuracy: 0.8043\nEpoch: 58 | Iteration: 50 | Loss: 1.098\nEpoch: 58 | Iteration: 100 | Loss: 1.073\nEpoch: 58 | Iteration: 150 | Loss: 1.114\nEpoch: 58 | Iteration: 200 | Loss: 0.998\nEpoch: 58 | Iteration: 250 | Loss: 1.096\nEpoch: 58 | Iteration: 300 | Loss: 1.105\nEpoch: 58 | Iteration: 350 | Loss: 1.158\nEpoch 58: Test accuracy: 0.7958\nEpoch: 59 | Iteration: 50 | Loss: 1.082\nEpoch: 59 | Iteration: 100 | Loss: 1.061\nEpoch: 59 | Iteration: 150 | Loss: 1.083\nEpoch: 59 | Iteration: 200 | Loss: 1.021\nEpoch: 59 | Iteration: 250 | Loss: 0.932\nEpoch: 59 | Iteration: 300 | Loss: 1.012\nEpoch: 59 | Iteration: 350 | Loss: 1.054\nEpoch 59: Test accuracy: 0.8293\nEpoch: 60 | Iteration: 50 | Loss: 1.028\nEpoch: 60 | Iteration: 100 | Loss: 1.074\nEpoch: 60 | Iteration: 150 | Loss: 1.179\nEpoch: 60 | Iteration: 200 | Loss: 1.038\nEpoch: 60 | Iteration: 250 | Loss: 1.142\nEpoch: 60 | Iteration: 300 | Loss: 0.997\nEpoch: 60 | Iteration: 350 | Loss: 0.882\nEpoch 60: Test accuracy: 0.8219\nEpoch: 61 | Iteration: 50 | Loss: 1.042\nEpoch: 61 | Iteration: 100 | Loss: 0.981\nEpoch: 61 | Iteration: 150 | Loss: 1.188\nEpoch: 61 | Iteration: 200 | Loss: 1.048\nEpoch: 61 | Iteration: 250 | Loss: 1.056\nEpoch: 61 | Iteration: 300 | Loss: 1.086\nEpoch: 61 | Iteration: 350 | Loss: 1.093\nEpoch 61: Test accuracy: 0.8467\nEpoch: 62 | Iteration: 50 | Loss: 1.023\nEpoch: 62 | Iteration: 100 | Loss: 1.016\nEpoch: 62 | Iteration: 150 | Loss: 1.117\nEpoch: 62 | Iteration: 200 | Loss: 0.957\nEpoch: 62 | Iteration: 250 | Loss: 0.946\nEpoch: 62 | Iteration: 300 | Loss: 1.042\nEpoch: 62 | Iteration: 350 | Loss: 1.041\nEpoch 62: Test accuracy: 0.8183\nEpoch: 63 | Iteration: 50 | Loss: 1.047\nEpoch: 63 | Iteration: 100 | Loss: 0.957\nEpoch: 63 | Iteration: 150 | Loss: 1.100\nEpoch: 63 | Iteration: 200 | Loss: 1.003\nEpoch: 63 | Iteration: 250 | Loss: 1.038\nEpoch: 63 | Iteration: 300 | Loss: 1.087\nEpoch: 63 | Iteration: 350 | Loss: 1.074\nEpoch 63: Test accuracy: 0.8265\nEpoch: 64 | Iteration: 50 | Loss: 0.958\nEpoch: 64 | Iteration: 100 | Loss: 1.052\nEpoch: 64 | Iteration: 150 | Loss: 1.073\nEpoch: 64 | Iteration: 200 | Loss: 1.046\nEpoch: 64 | Iteration: 250 | Loss: 1.069\nEpoch: 64 | Iteration: 300 | Loss: 0.981\nEpoch: 64 | Iteration: 350 | Loss: 0.989\nEpoch 64: Test accuracy: 0.8176\nEpoch: 65 | Iteration: 50 | Loss: 1.043\nEpoch: 65 | Iteration: 100 | Loss: 1.078\nEpoch: 65 | Iteration: 150 | Loss: 1.001\nEpoch: 65 | Iteration: 200 | Loss: 1.043\nEpoch: 65 | Iteration: 250 | Loss: 1.004\nEpoch: 65 | Iteration: 300 | Loss: 1.056\nEpoch: 65 | Iteration: 350 | Loss: 1.097\nEpoch 65: Test accuracy: 0.8285\nEpoch: 66 | Iteration: 50 | Loss: 1.069\nEpoch: 66 | Iteration: 100 | Loss: 1.034\nEpoch: 66 | Iteration: 150 | Loss: 0.979\nEpoch: 66 | Iteration: 200 | Loss: 1.025\nEpoch: 66 | Iteration: 250 | Loss: 1.135\nEpoch: 66 | Iteration: 300 | Loss: 0.963\nEpoch: 66 | Iteration: 350 | Loss: 1.031\nEpoch 66: Test accuracy: 0.8312\nEpoch: 67 | Iteration: 50 | Loss: 0.983\nEpoch: 67 | Iteration: 100 | Loss: 1.055\nEpoch: 67 | Iteration: 150 | Loss: 0.978\nEpoch: 67 | Iteration: 200 | Loss: 1.057\nEpoch: 67 | Iteration: 250 | Loss: 1.061\nEpoch: 67 | Iteration: 300 | Loss: 0.988\nEpoch: 67 | Iteration: 350 | Loss: 1.019\nEpoch 67: Test accuracy: 0.8416\nEpoch: 68 | Iteration: 50 | Loss: 1.007\nEpoch: 68 | Iteration: 100 | Loss: 1.186\nEpoch: 68 | Iteration: 150 | Loss: 0.976\nEpoch: 68 | Iteration: 200 | Loss: 1.061\nEpoch: 68 | Iteration: 250 | Loss: 0.979\nEpoch: 68 | Iteration: 300 | Loss: 0.949\nEpoch: 68 | Iteration: 350 | Loss: 1.028\nEpoch 68: Test accuracy: 0.8339\nEpoch: 69 | Iteration: 50 | Loss: 1.024\nEpoch: 69 | Iteration: 100 | Loss: 0.987\nEpoch: 69 | Iteration: 150 | Loss: 1.058\nEpoch: 69 | Iteration: 200 | Loss: 0.950\nEpoch: 69 | Iteration: 250 | Loss: 1.047\nEpoch: 69 | Iteration: 300 | Loss: 1.051\nEpoch: 69 | Iteration: 350 | Loss: 0.923\nEpoch 69: Test accuracy: 0.8346\nEpoch: 70 | Iteration: 50 | Loss: 1.082\nEpoch: 70 | Iteration: 100 | Loss: 1.037\nEpoch: 70 | Iteration: 150 | Loss: 1.069\nEpoch: 70 | Iteration: 200 | Loss: 0.991\nEpoch: 70 | Iteration: 250 | Loss: 1.070\nEpoch: 70 | Iteration: 300 | Loss: 0.951\nEpoch: 70 | Iteration: 350 | Loss: 1.043\nEpoch 70: Test accuracy: 0.8188\nEpoch: 71 | Iteration: 50 | Loss: 0.993\nEpoch: 71 | Iteration: 100 | Loss: 0.996\nEpoch: 71 | Iteration: 150 | Loss: 1.121\nEpoch: 71 | Iteration: 200 | Loss: 0.994\nEpoch: 71 | Iteration: 250 | Loss: 1.061\nEpoch: 71 | Iteration: 300 | Loss: 1.111\nEpoch: 71 | Iteration: 350 | Loss: 0.991\nEpoch 71: Test accuracy: 0.8305\nEpoch: 72 | Iteration: 50 | Loss: 1.084\nEpoch: 72 | Iteration: 100 | Loss: 1.084\nEpoch: 72 | Iteration: 150 | Loss: 1.054\nEpoch: 72 | Iteration: 200 | Loss: 1.054\nEpoch: 72 | Iteration: 250 | Loss: 1.066\nEpoch: 72 | Iteration: 300 | Loss: 1.080\nEpoch: 72 | Iteration: 350 | Loss: 0.972\nEpoch 72: Test accuracy: 0.8344\nEpoch: 73 | Iteration: 50 | Loss: 1.062\nEpoch: 73 | Iteration: 100 | Loss: 1.025\nEpoch: 73 | Iteration: 150 | Loss: 1.057\nEpoch: 73 | Iteration: 200 | Loss: 1.057\nEpoch: 73 | Iteration: 250 | Loss: 1.048\nEpoch: 73 | Iteration: 300 | Loss: 0.982\nEpoch: 73 | Iteration: 350 | Loss: 0.961\nEpoch 73: Test accuracy: 0.8378\nEpoch: 74 | Iteration: 50 | Loss: 1.003\nEpoch: 74 | Iteration: 100 | Loss: 0.964\nEpoch: 74 | Iteration: 150 | Loss: 1.047\nEpoch: 74 | Iteration: 200 | Loss: 1.000\nEpoch: 74 | Iteration: 250 | Loss: 1.047\nEpoch: 74 | Iteration: 300 | Loss: 1.101\nEpoch: 74 | Iteration: 350 | Loss: 0.978\nEpoch 74: Test accuracy: 0.8094\nEpoch: 75 | Iteration: 50 | Loss: 1.041\nEpoch: 75 | Iteration: 100 | Loss: 1.054\nEpoch: 75 | Iteration: 150 | Loss: 1.016\nEpoch: 75 | Iteration: 200 | Loss: 1.116\nEpoch: 75 | Iteration: 250 | Loss: 1.038\nEpoch: 75 | Iteration: 300 | Loss: 1.007\nEpoch: 75 | Iteration: 350 | Loss: 1.061\nEpoch 75: Test accuracy: 0.8181\nEpoch: 76 | Iteration: 50 | Loss: 1.004\nEpoch: 76 | Iteration: 100 | Loss: 1.141\nEpoch: 76 | Iteration: 150 | Loss: 1.109\nEpoch: 76 | Iteration: 200 | Loss: 0.990\nEpoch: 76 | Iteration: 250 | Loss: 0.968\nEpoch: 76 | Iteration: 300 | Loss: 1.036\nEpoch: 76 | Iteration: 350 | Loss: 1.024\nEpoch 76: Test accuracy: 0.8388\nEpoch: 77 | Iteration: 50 | Loss: 1.018\nEpoch: 77 | Iteration: 100 | Loss: 0.992\nEpoch: 77 | Iteration: 150 | Loss: 1.061\nEpoch: 77 | Iteration: 200 | Loss: 1.079\nEpoch: 77 | Iteration: 250 | Loss: 0.949\nEpoch: 77 | Iteration: 300 | Loss: 1.115\nEpoch: 77 | Iteration: 350 | Loss: 0.968\nEpoch 77: Test accuracy: 0.8491\nEpoch: 78 | Iteration: 50 | Loss: 0.893\nEpoch: 78 | Iteration: 100 | Loss: 1.026\nEpoch: 78 | Iteration: 150 | Loss: 0.984\nEpoch: 78 | Iteration: 200 | Loss: 1.067\nEpoch: 78 | Iteration: 250 | Loss: 1.081\nEpoch: 78 | Iteration: 300 | Loss: 1.007\nEpoch: 78 | Iteration: 350 | Loss: 1.141\nEpoch 78: Test accuracy: 0.8049\nEpoch: 79 | Iteration: 50 | Loss: 1.055\nEpoch: 79 | Iteration: 100 | Loss: 1.055\nEpoch: 79 | Iteration: 150 | Loss: 1.106\nEpoch: 79 | Iteration: 200 | Loss: 1.071\nEpoch: 79 | Iteration: 250 | Loss: 1.012\nEpoch: 79 | Iteration: 300 | Loss: 1.120\nEpoch: 79 | Iteration: 350 | Loss: 1.072\nEpoch 79: Test accuracy: 0.8487\nEpoch: 80 | Iteration: 50 | Loss: 1.011\nEpoch: 80 | Iteration: 100 | Loss: 1.068\nEpoch: 80 | Iteration: 150 | Loss: 0.973\nEpoch: 80 | Iteration: 200 | Loss: 1.045\nEpoch: 80 | Iteration: 250 | Loss: 0.960\nEpoch: 80 | Iteration: 300 | Loss: 1.003\nEpoch: 80 | Iteration: 350 | Loss: 1.100\nEpoch 80: Test accuracy: 0.8214\nEpoch: 81 | Iteration: 50 | Loss: 1.069\nEpoch: 81 | Iteration: 100 | Loss: 1.032\nEpoch: 81 | Iteration: 150 | Loss: 0.974\nEpoch: 81 | Iteration: 200 | Loss: 1.118\nEpoch: 81 | Iteration: 250 | Loss: 1.026\nEpoch: 81 | Iteration: 300 | Loss: 0.900\nEpoch: 81 | Iteration: 350 | Loss: 1.085\nEpoch 81: Test accuracy: 0.8355\nEpoch: 82 | Iteration: 50 | Loss: 0.996\nEpoch: 82 | Iteration: 100 | Loss: 0.946\nEpoch: 82 | Iteration: 150 | Loss: 1.041\nEpoch: 82 | Iteration: 200 | Loss: 0.949\nEpoch: 82 | Iteration: 250 | Loss: 1.099\nEpoch: 82 | Iteration: 300 | Loss: 0.987\nEpoch: 82 | Iteration: 350 | Loss: 1.055\nEpoch 82: Test accuracy: 0.7657\nEpoch: 83 | Iteration: 50 | Loss: 1.075\nEpoch: 83 | Iteration: 100 | Loss: 1.041\nEpoch: 83 | Iteration: 150 | Loss: 1.066\nEpoch: 83 | Iteration: 200 | Loss: 1.036\nEpoch: 83 | Iteration: 250 | Loss: 1.062\nEpoch: 83 | Iteration: 300 | Loss: 0.942\nEpoch: 83 | Iteration: 350 | Loss: 1.162\nEpoch 83: Test accuracy: 0.7860\nEpoch: 84 | Iteration: 50 | Loss: 1.036\nEpoch: 84 | Iteration: 100 | Loss: 1.106\nEpoch: 84 | Iteration: 150 | Loss: 1.052\nEpoch: 84 | Iteration: 200 | Loss: 1.017\nEpoch: 84 | Iteration: 250 | Loss: 1.023\nEpoch: 84 | Iteration: 300 | Loss: 1.042\nEpoch: 84 | Iteration: 350 | Loss: 1.036\nEpoch 84: Test accuracy: 0.8208\nEpoch: 85 | Iteration: 50 | Loss: 1.015\nEpoch: 85 | Iteration: 100 | Loss: 1.041\nEpoch: 85 | Iteration: 150 | Loss: 1.028\nEpoch: 85 | Iteration: 200 | Loss: 0.919\nEpoch: 85 | Iteration: 250 | Loss: 0.977\nEpoch: 85 | Iteration: 300 | Loss: 0.982\nEpoch: 85 | Iteration: 350 | Loss: 0.896\nEpoch 85: Test accuracy: 0.8584\nEpoch: 86 | Iteration: 50 | Loss: 0.999\nEpoch: 86 | Iteration: 100 | Loss: 0.949\nEpoch: 86 | Iteration: 150 | Loss: 1.086\nEpoch: 86 | Iteration: 200 | Loss: 0.960\nEpoch: 86 | Iteration: 250 | Loss: 0.997\nEpoch: 86 | Iteration: 300 | Loss: 1.033\nEpoch: 86 | Iteration: 350 | Loss: 0.978\nEpoch 86: Test accuracy: 0.8435\nEpoch: 87 | Iteration: 50 | Loss: 0.973\nEpoch: 87 | Iteration: 100 | Loss: 1.040\nEpoch: 87 | Iteration: 150 | Loss: 1.047\nEpoch: 87 | Iteration: 200 | Loss: 1.053\nEpoch: 87 | Iteration: 250 | Loss: 0.891\nEpoch: 87 | Iteration: 300 | Loss: 0.961\nEpoch: 87 | Iteration: 350 | Loss: 1.057\nEpoch 87: Test accuracy: 0.8085\nEpoch: 88 | Iteration: 50 | Loss: 1.037\nEpoch: 88 | Iteration: 100 | Loss: 1.046\nEpoch: 88 | Iteration: 150 | Loss: 1.027\nEpoch: 88 | Iteration: 200 | Loss: 0.987\nEpoch: 88 | Iteration: 250 | Loss: 1.021\nEpoch: 88 | Iteration: 300 | Loss: 0.970\nEpoch: 88 | Iteration: 350 | Loss: 1.026\nEpoch 88: Test accuracy: 0.8621\nEpoch: 89 | Iteration: 50 | Loss: 0.998\nEpoch: 89 | Iteration: 100 | Loss: 1.007\nEpoch: 89 | Iteration: 150 | Loss: 0.993\nEpoch: 89 | Iteration: 200 | Loss: 1.102\nEpoch: 89 | Iteration: 250 | Loss: 1.028\nEpoch: 89 | Iteration: 300 | Loss: 0.957\nEpoch: 89 | Iteration: 350 | Loss: 0.988\nEpoch 89: Test accuracy: 0.8531\nEpoch: 90 | Iteration: 50 | Loss: 0.986\nEpoch: 90 | Iteration: 100 | Loss: 1.008\nEpoch: 90 | Iteration: 150 | Loss: 1.045\nEpoch: 90 | Iteration: 200 | Loss: 0.954\nEpoch: 90 | Iteration: 250 | Loss: 0.921\nEpoch: 90 | Iteration: 300 | Loss: 1.000\nEpoch: 90 | Iteration: 350 | Loss: 0.921\nEpoch 90: Test accuracy: 0.8448\nEpoch: 91 | Iteration: 50 | Loss: 0.993\nEpoch: 91 | Iteration: 100 | Loss: 0.966\nEpoch: 91 | Iteration: 150 | Loss: 0.966\nEpoch: 91 | Iteration: 200 | Loss: 0.940\nEpoch: 91 | Iteration: 250 | Loss: 1.079\nEpoch: 91 | Iteration: 300 | Loss: 0.956\nEpoch: 91 | Iteration: 350 | Loss: 0.885\nEpoch 91: Test accuracy: 0.8282\nEpoch: 92 | Iteration: 50 | Loss: 0.978\nEpoch: 92 | Iteration: 100 | Loss: 1.062\nEpoch: 92 | Iteration: 150 | Loss: 0.979\nEpoch: 92 | Iteration: 200 | Loss: 1.016\nEpoch: 92 | Iteration: 250 | Loss: 0.999\nEpoch: 92 | Iteration: 300 | Loss: 0.874\nEpoch: 92 | Iteration: 350 | Loss: 1.109\nEpoch 92: Test accuracy: 0.8372\nEpoch: 93 | Iteration: 50 | Loss: 0.939\nEpoch: 93 | Iteration: 100 | Loss: 0.871\nEpoch: 93 | Iteration: 150 | Loss: 0.925\nEpoch: 93 | Iteration: 200 | Loss: 0.973\nEpoch: 93 | Iteration: 250 | Loss: 1.013\nEpoch: 93 | Iteration: 300 | Loss: 1.019\nEpoch: 93 | Iteration: 350 | Loss: 0.969\nEpoch 93: Test accuracy: 0.8103\nEpoch: 94 | Iteration: 50 | Loss: 1.125\nEpoch: 94 | Iteration: 100 | Loss: 1.072\nEpoch: 94 | Iteration: 150 | Loss: 0.954\nEpoch: 94 | Iteration: 200 | Loss: 1.008\nEpoch: 94 | Iteration: 250 | Loss: 0.858\nEpoch: 94 | Iteration: 300 | Loss: 0.975\nEpoch: 94 | Iteration: 350 | Loss: 0.905\nEpoch 94: Test accuracy: 0.8668\nEpoch: 95 | Iteration: 50 | Loss: 0.991\nEpoch: 95 | Iteration: 100 | Loss: 0.923\nEpoch: 95 | Iteration: 150 | Loss: 0.953\nEpoch: 95 | Iteration: 200 | Loss: 0.929\nEpoch: 95 | Iteration: 250 | Loss: 0.995\nEpoch: 95 | Iteration: 300 | Loss: 0.865\nEpoch: 95 | Iteration: 350 | Loss: 1.016\nEpoch 95: Test accuracy: 0.8582\nEpoch: 96 | Iteration: 50 | Loss: 0.978\nEpoch: 96 | Iteration: 100 | Loss: 0.875\nEpoch: 96 | Iteration: 150 | Loss: 0.890\nEpoch: 96 | Iteration: 200 | Loss: 0.944\nEpoch: 96 | Iteration: 250 | Loss: 0.925\nEpoch: 96 | Iteration: 300 | Loss: 0.994\nEpoch: 96 | Iteration: 350 | Loss: 1.002\nEpoch 96: Test accuracy: 0.8458\nEpoch: 97 | Iteration: 50 | Loss: 0.953\nEpoch: 97 | Iteration: 100 | Loss: 0.967\nEpoch: 97 | Iteration: 150 | Loss: 0.972\nEpoch: 97 | Iteration: 200 | Loss: 0.950\nEpoch: 97 | Iteration: 250 | Loss: 0.969\nEpoch: 97 | Iteration: 300 | Loss: 0.995\nEpoch: 97 | Iteration: 350 | Loss: 1.010\nEpoch 97: Test accuracy: 0.8540\nEpoch: 98 | Iteration: 50 | Loss: 0.964\nEpoch: 98 | Iteration: 100 | Loss: 0.996\nEpoch: 98 | Iteration: 150 | Loss: 0.980\nEpoch: 98 | Iteration: 200 | Loss: 1.027\nEpoch: 98 | Iteration: 250 | Loss: 1.025\nEpoch: 98 | Iteration: 300 | Loss: 1.037\nEpoch: 98 | Iteration: 350 | Loss: 1.008\nEpoch 98: Test accuracy: 0.8613\nEpoch: 99 | Iteration: 50 | Loss: 0.994\nEpoch: 99 | Iteration: 100 | Loss: 1.006\nEpoch: 99 | Iteration: 150 | Loss: 0.931\nEpoch: 99 | Iteration: 200 | Loss: 1.123\nEpoch: 99 | Iteration: 250 | Loss: 0.831\nEpoch: 99 | Iteration: 300 | Loss: 1.085\nEpoch: 99 | Iteration: 350 | Loss: 0.942\nEpoch 99: Test accuracy: 0.8557\nEpoch: 100 | Iteration: 50 | Loss: 0.944\nEpoch: 100 | Iteration: 100 | Loss: 0.895\nEpoch: 100 | Iteration: 150 | Loss: 0.913\nEpoch: 100 | Iteration: 200 | Loss: 0.969\nEpoch: 100 | Iteration: 250 | Loss: 0.990\nEpoch: 100 | Iteration: 300 | Loss: 0.967\nEpoch: 100 | Iteration: 350 | Loss: 0.905\nEpoch 100: Test accuracy: 0.8712\nEpoch: 101 | Iteration: 50 | Loss: 0.900\nEpoch: 101 | Iteration: 100 | Loss: 0.947\nEpoch: 101 | Iteration: 150 | Loss: 0.910\nEpoch: 101 | Iteration: 200 | Loss: 0.960\nEpoch: 101 | Iteration: 250 | Loss: 0.984\nEpoch: 101 | Iteration: 300 | Loss: 0.918\nEpoch: 101 | Iteration: 350 | Loss: 1.096\nEpoch 101: Test accuracy: 0.8500\nEpoch: 102 | Iteration: 50 | Loss: 0.970\nEpoch: 102 | Iteration: 100 | Loss: 0.997\nEpoch: 102 | Iteration: 150 | Loss: 0.952\nEpoch: 102 | Iteration: 200 | Loss: 0.962\nEpoch: 102 | Iteration: 250 | Loss: 0.844\nEpoch: 102 | Iteration: 300 | Loss: 1.064\nEpoch: 102 | Iteration: 350 | Loss: 1.011\nEpoch 102: Test accuracy: 0.8770\nEpoch: 103 | Iteration: 50 | Loss: 0.936\nEpoch: 103 | Iteration: 100 | Loss: 0.903\nEpoch: 103 | Iteration: 150 | Loss: 1.074\nEpoch: 103 | Iteration: 200 | Loss: 1.000\nEpoch: 103 | Iteration: 250 | Loss: 1.011\nEpoch: 103 | Iteration: 300 | Loss: 0.884\nEpoch: 103 | Iteration: 350 | Loss: 0.932\nEpoch 103: Test accuracy: 0.8412\nEpoch: 104 | Iteration: 50 | Loss: 0.987\nEpoch: 104 | Iteration: 100 | Loss: 1.076\nEpoch: 104 | Iteration: 150 | Loss: 0.901\nEpoch: 104 | Iteration: 200 | Loss: 0.907\nEpoch: 104 | Iteration: 250 | Loss: 0.902\nEpoch: 104 | Iteration: 300 | Loss: 0.977\nEpoch: 104 | Iteration: 350 | Loss: 1.063\nEpoch 104: Test accuracy: 0.8580\nEpoch: 105 | Iteration: 50 | Loss: 0.899\nEpoch: 105 | Iteration: 100 | Loss: 1.031\nEpoch: 105 | Iteration: 150 | Loss: 1.057\nEpoch: 105 | Iteration: 200 | Loss: 1.014\nEpoch: 105 | Iteration: 250 | Loss: 0.993\nEpoch: 105 | Iteration: 300 | Loss: 0.991\nEpoch: 105 | Iteration: 350 | Loss: 0.979\nEpoch 105: Test accuracy: 0.8764\nEpoch: 106 | Iteration: 50 | Loss: 0.923\nEpoch: 106 | Iteration: 100 | Loss: 0.928\nEpoch: 106 | Iteration: 150 | Loss: 0.991\nEpoch: 106 | Iteration: 200 | Loss: 0.985\nEpoch: 106 | Iteration: 250 | Loss: 0.910\nEpoch: 106 | Iteration: 300 | Loss: 1.044\nEpoch: 106 | Iteration: 350 | Loss: 0.981\nEpoch 106: Test accuracy: 0.8607\nEpoch: 107 | Iteration: 50 | Loss: 0.863\nEpoch: 107 | Iteration: 100 | Loss: 1.041\nEpoch: 107 | Iteration: 150 | Loss: 1.038\nEpoch: 107 | Iteration: 200 | Loss: 1.091\nEpoch: 107 | Iteration: 250 | Loss: 0.972\nEpoch: 107 | Iteration: 300 | Loss: 0.981\nEpoch: 107 | Iteration: 350 | Loss: 0.923\nEpoch 107: Test accuracy: 0.8594\nEpoch: 108 | Iteration: 50 | Loss: 0.981\nEpoch: 108 | Iteration: 100 | Loss: 0.855\nEpoch: 108 | Iteration: 150 | Loss: 1.025\nEpoch: 108 | Iteration: 200 | Loss: 0.999\nEpoch: 108 | Iteration: 250 | Loss: 0.994\nEpoch: 108 | Iteration: 300 | Loss: 0.848\nEpoch: 108 | Iteration: 350 | Loss: 0.989\nEpoch 108: Test accuracy: 0.8284\nEpoch: 109 | Iteration: 50 | Loss: 0.881\nEpoch: 109 | Iteration: 100 | Loss: 0.966\nEpoch: 109 | Iteration: 150 | Loss: 0.854\nEpoch: 109 | Iteration: 200 | Loss: 0.941\nEpoch: 109 | Iteration: 250 | Loss: 0.966\nEpoch: 109 | Iteration: 300 | Loss: 0.944\nEpoch: 109 | Iteration: 350 | Loss: 0.899\nEpoch 109: Test accuracy: 0.8533\nEpoch: 110 | Iteration: 50 | Loss: 0.984\nEpoch: 110 | Iteration: 100 | Loss: 0.858\nEpoch: 110 | Iteration: 150 | Loss: 0.992\nEpoch: 110 | Iteration: 200 | Loss: 0.913\nEpoch: 110 | Iteration: 250 | Loss: 0.914\nEpoch: 110 | Iteration: 300 | Loss: 0.858\nEpoch: 110 | Iteration: 350 | Loss: 1.019\nEpoch 110: Test accuracy: 0.8899\nEpoch: 111 | Iteration: 50 | Loss: 0.803\nEpoch: 111 | Iteration: 100 | Loss: 0.895\nEpoch: 111 | Iteration: 150 | Loss: 0.815\nEpoch: 111 | Iteration: 200 | Loss: 0.902\nEpoch: 111 | Iteration: 250 | Loss: 0.934\nEpoch: 111 | Iteration: 300 | Loss: 0.887\nEpoch: 111 | Iteration: 350 | Loss: 0.981\nEpoch 111: Test accuracy: 0.8695\nEpoch: 112 | Iteration: 50 | Loss: 0.947\nEpoch: 112 | Iteration: 100 | Loss: 0.841\nEpoch: 112 | Iteration: 150 | Loss: 0.952\nEpoch: 112 | Iteration: 200 | Loss: 0.839\nEpoch: 112 | Iteration: 250 | Loss: 0.883\nEpoch: 112 | Iteration: 300 | Loss: 0.859\nEpoch: 112 | Iteration: 350 | Loss: 0.892\nEpoch 112: Test accuracy: 0.8714\nEpoch: 113 | Iteration: 50 | Loss: 1.062\nEpoch: 113 | Iteration: 100 | Loss: 0.920\nEpoch: 113 | Iteration: 150 | Loss: 0.934\nEpoch: 113 | Iteration: 200 | Loss: 0.895\nEpoch: 113 | Iteration: 250 | Loss: 0.910\nEpoch: 113 | Iteration: 300 | Loss: 1.014\nEpoch: 113 | Iteration: 350 | Loss: 0.951\nEpoch 113: Test accuracy: 0.8802\nEpoch: 114 | Iteration: 50 | Loss: 0.978\nEpoch: 114 | Iteration: 100 | Loss: 0.905\nEpoch: 114 | Iteration: 150 | Loss: 0.841\nEpoch: 114 | Iteration: 200 | Loss: 1.026\nEpoch: 114 | Iteration: 250 | Loss: 1.003\nEpoch: 114 | Iteration: 300 | Loss: 0.956\nEpoch: 114 | Iteration: 350 | Loss: 0.914\nEpoch 114: Test accuracy: 0.8886\nEpoch: 115 | Iteration: 50 | Loss: 0.851\nEpoch: 115 | Iteration: 100 | Loss: 0.980\nEpoch: 115 | Iteration: 150 | Loss: 0.936\nEpoch: 115 | Iteration: 200 | Loss: 0.965\nEpoch: 115 | Iteration: 250 | Loss: 0.895\nEpoch: 115 | Iteration: 300 | Loss: 0.841\nEpoch: 115 | Iteration: 350 | Loss: 0.977\nEpoch 115: Test accuracy: 0.8877\nEpoch: 116 | Iteration: 50 | Loss: 0.872\nEpoch: 116 | Iteration: 100 | Loss: 0.871\nEpoch: 116 | Iteration: 150 | Loss: 0.788\nEpoch: 116 | Iteration: 200 | Loss: 0.966\nEpoch: 116 | Iteration: 250 | Loss: 0.940\nEpoch: 116 | Iteration: 300 | Loss: 1.009\nEpoch: 116 | Iteration: 350 | Loss: 0.840\nEpoch 116: Test accuracy: 0.8815\nEpoch: 117 | Iteration: 50 | Loss: 1.030\nEpoch: 117 | Iteration: 100 | Loss: 0.898\nEpoch: 117 | Iteration: 150 | Loss: 1.013\nEpoch: 117 | Iteration: 200 | Loss: 0.848\nEpoch: 117 | Iteration: 250 | Loss: 0.938\nEpoch: 117 | Iteration: 300 | Loss: 0.891\nEpoch: 117 | Iteration: 350 | Loss: 0.949\nEpoch 117: Test accuracy: 0.8784\nEpoch: 118 | Iteration: 50 | Loss: 0.849\nEpoch: 118 | Iteration: 100 | Loss: 0.895\nEpoch: 118 | Iteration: 150 | Loss: 0.852\nEpoch: 118 | Iteration: 200 | Loss: 0.824\nEpoch: 118 | Iteration: 250 | Loss: 1.078\nEpoch: 118 | Iteration: 300 | Loss: 0.948\nEpoch: 118 | Iteration: 350 | Loss: 0.786\nEpoch 118: Test accuracy: 0.8846\nEpoch: 119 | Iteration: 50 | Loss: 1.006\nEpoch: 119 | Iteration: 100 | Loss: 0.935\nEpoch: 119 | Iteration: 150 | Loss: 0.859\nEpoch: 119 | Iteration: 200 | Loss: 0.837\nEpoch: 119 | Iteration: 250 | Loss: 1.000\nEpoch: 119 | Iteration: 300 | Loss: 1.030\nEpoch: 119 | Iteration: 350 | Loss: 0.849\nEpoch 119: Test accuracy: 0.8921\nEpoch: 120 | Iteration: 50 | Loss: 0.761\nEpoch: 120 | Iteration: 100 | Loss: 0.946\nEpoch: 120 | Iteration: 150 | Loss: 0.920\nEpoch: 120 | Iteration: 200 | Loss: 0.795\nEpoch: 120 | Iteration: 250 | Loss: 0.864\nEpoch: 120 | Iteration: 300 | Loss: 0.963\nEpoch: 120 | Iteration: 350 | Loss: 0.872\nEpoch 120: Test accuracy: 0.8931\nEpoch: 121 | Iteration: 50 | Loss: 0.806\nEpoch: 121 | Iteration: 100 | Loss: 0.945\nEpoch: 121 | Iteration: 150 | Loss: 0.887\nEpoch: 121 | Iteration: 200 | Loss: 0.773\nEpoch: 121 | Iteration: 250 | Loss: 0.840\nEpoch: 121 | Iteration: 300 | Loss: 0.850\nEpoch: 121 | Iteration: 350 | Loss: 0.821\nEpoch 121: Test accuracy: 0.8884\nEpoch: 122 | Iteration: 50 | Loss: 0.865\nEpoch: 122 | Iteration: 100 | Loss: 0.797\nEpoch: 122 | Iteration: 150 | Loss: 0.829\nEpoch: 122 | Iteration: 200 | Loss: 0.850\nEpoch: 122 | Iteration: 250 | Loss: 0.927\nEpoch: 122 | Iteration: 300 | Loss: 0.873\nEpoch: 122 | Iteration: 350 | Loss: 0.824\nEpoch 122: Test accuracy: 0.9010\nEpoch: 123 | Iteration: 50 | Loss: 0.868\nEpoch: 123 | Iteration: 100 | Loss: 0.941\nEpoch: 123 | Iteration: 150 | Loss: 0.785\nEpoch: 123 | Iteration: 200 | Loss: 0.826\nEpoch: 123 | Iteration: 250 | Loss: 0.838\nEpoch: 123 | Iteration: 300 | Loss: 0.893\nEpoch: 123 | Iteration: 350 | Loss: 0.939\nEpoch 123: Test accuracy: 0.8970\nEpoch: 124 | Iteration: 50 | Loss: 0.867\nEpoch: 124 | Iteration: 100 | Loss: 0.859\nEpoch: 124 | Iteration: 150 | Loss: 0.811\nEpoch: 124 | Iteration: 200 | Loss: 0.867\nEpoch: 124 | Iteration: 250 | Loss: 0.861\nEpoch: 124 | Iteration: 300 | Loss: 0.895\nEpoch: 124 | Iteration: 350 | Loss: 1.014\nEpoch 124: Test accuracy: 0.9019\nEpoch: 125 | Iteration: 50 | Loss: 0.760\nEpoch: 125 | Iteration: 100 | Loss: 0.943\nEpoch: 125 | Iteration: 150 | Loss: 0.826\nEpoch: 125 | Iteration: 200 | Loss: 0.838\nEpoch: 125 | Iteration: 250 | Loss: 0.777\nEpoch: 125 | Iteration: 300 | Loss: 0.895\nEpoch: 125 | Iteration: 350 | Loss: 0.772\nEpoch 125: Test accuracy: 0.9021\nEpoch: 126 | Iteration: 50 | Loss: 0.813\nEpoch: 126 | Iteration: 100 | Loss: 0.727\nEpoch: 126 | Iteration: 150 | Loss: 0.790\nEpoch: 126 | Iteration: 200 | Loss: 0.832\nEpoch: 126 | Iteration: 250 | Loss: 0.755\nEpoch: 126 | Iteration: 300 | Loss: 0.907\nEpoch: 126 | Iteration: 350 | Loss: 0.904\nEpoch 126: Test accuracy: 0.9018\nEpoch: 127 | Iteration: 50 | Loss: 0.846\nEpoch: 127 | Iteration: 100 | Loss: 0.764\nEpoch: 127 | Iteration: 150 | Loss: 0.777\nEpoch: 127 | Iteration: 200 | Loss: 0.958\nEpoch: 127 | Iteration: 250 | Loss: 0.897\nEpoch: 127 | Iteration: 300 | Loss: 0.785\nEpoch: 127 | Iteration: 350 | Loss: 0.860\nEpoch 127: Test accuracy: 0.9136\nEpoch: 128 | Iteration: 50 | Loss: 0.913\nEpoch: 128 | Iteration: 100 | Loss: 0.787\nEpoch: 128 | Iteration: 150 | Loss: 0.799\nEpoch: 128 | Iteration: 200 | Loss: 0.767\nEpoch: 128 | Iteration: 250 | Loss: 0.820\nEpoch: 128 | Iteration: 300 | Loss: 0.886\nEpoch: 128 | Iteration: 350 | Loss: 0.828\nEpoch 128: Test accuracy: 0.9125\nEpoch: 129 | Iteration: 50 | Loss: 0.783\nEpoch: 129 | Iteration: 100 | Loss: 0.743\nEpoch: 129 | Iteration: 150 | Loss: 0.801\nEpoch: 129 | Iteration: 200 | Loss: 0.858\nEpoch: 129 | Iteration: 250 | Loss: 0.749\nEpoch: 129 | Iteration: 300 | Loss: 0.905\nEpoch: 129 | Iteration: 350 | Loss: 0.776\nEpoch 129: Test accuracy: 0.9183\nEpoch: 130 | Iteration: 50 | Loss: 0.809\nEpoch: 130 | Iteration: 100 | Loss: 0.819\nEpoch: 130 | Iteration: 150 | Loss: 0.744\nEpoch: 130 | Iteration: 200 | Loss: 0.796\nEpoch: 130 | Iteration: 250 | Loss: 0.748\nEpoch: 130 | Iteration: 300 | Loss: 0.762\nEpoch: 130 | Iteration: 350 | Loss: 0.733\nEpoch 130: Test accuracy: 0.9200\nEpoch: 131 | Iteration: 50 | Loss: 0.716\nEpoch: 131 | Iteration: 100 | Loss: 0.655\nEpoch: 131 | Iteration: 150 | Loss: 0.883\nEpoch: 131 | Iteration: 200 | Loss: 0.852\nEpoch: 131 | Iteration: 250 | Loss: 0.813\nEpoch: 131 | Iteration: 300 | Loss: 0.827\nEpoch: 131 | Iteration: 350 | Loss: 0.767\nEpoch 131: Test accuracy: 0.9236\nEpoch: 132 | Iteration: 50 | Loss: 0.777\nEpoch: 132 | Iteration: 100 | Loss: 0.821\nEpoch: 132 | Iteration: 150 | Loss: 0.725\nEpoch: 132 | Iteration: 200 | Loss: 0.861\nEpoch: 132 | Iteration: 250 | Loss: 0.733\nEpoch: 132 | Iteration: 300 | Loss: 0.799\nEpoch: 132 | Iteration: 350 | Loss: 0.811\nEpoch 132: Test accuracy: 0.9223\nEpoch: 133 | Iteration: 50 | Loss: 0.803\nEpoch: 133 | Iteration: 100 | Loss: 0.767\nEpoch: 133 | Iteration: 150 | Loss: 0.739\nEpoch: 133 | Iteration: 200 | Loss: 0.764\nEpoch: 133 | Iteration: 250 | Loss: 0.735\nEpoch: 133 | Iteration: 300 | Loss: 0.782\nEpoch: 133 | Iteration: 350 | Loss: 0.709\nEpoch 133: Test accuracy: 0.9198\nEpoch: 134 | Iteration: 50 | Loss: 0.842\nEpoch: 134 | Iteration: 100 | Loss: 0.814\nEpoch: 134 | Iteration: 150 | Loss: 0.653\nEpoch: 134 | Iteration: 200 | Loss: 0.776\nEpoch: 134 | Iteration: 250 | Loss: 0.821\nEpoch: 134 | Iteration: 300 | Loss: 0.925\nEpoch: 134 | Iteration: 350 | Loss: 0.746\nEpoch 134: Test accuracy: 0.9229\nEpoch: 135 | Iteration: 50 | Loss: 0.824\nEpoch: 135 | Iteration: 100 | Loss: 0.794\nEpoch: 135 | Iteration: 150 | Loss: 0.728\nEpoch: 135 | Iteration: 200 | Loss: 0.768\nEpoch: 135 | Iteration: 250 | Loss: 0.745\nEpoch: 135 | Iteration: 300 | Loss: 0.715\nEpoch: 135 | Iteration: 350 | Loss: 0.788\nEpoch 135: Test accuracy: 0.9277\nEpoch: 136 | Iteration: 50 | Loss: 0.636\nEpoch: 136 | Iteration: 100 | Loss: 0.652\nEpoch: 136 | Iteration: 150 | Loss: 0.668\nEpoch: 136 | Iteration: 200 | Loss: 0.752\nEpoch: 136 | Iteration: 250 | Loss: 0.644\nEpoch: 136 | Iteration: 300 | Loss: 0.801\nEpoch: 136 | Iteration: 350 | Loss: 0.621\nEpoch 136: Test accuracy: 0.9348\nEpoch: 137 | Iteration: 50 | Loss: 0.857\nEpoch: 137 | Iteration: 100 | Loss: 0.722\nEpoch: 137 | Iteration: 150 | Loss: 0.657\nEpoch: 137 | Iteration: 200 | Loss: 0.745\nEpoch: 137 | Iteration: 250 | Loss: 0.740\nEpoch: 137 | Iteration: 300 | Loss: 0.654\nEpoch: 137 | Iteration: 350 | Loss: 0.741\nEpoch 137: Test accuracy: 0.9366\nEpoch: 138 | Iteration: 50 | Loss: 0.733\nEpoch: 138 | Iteration: 100 | Loss: 0.731\nEpoch: 138 | Iteration: 150 | Loss: 0.680\nEpoch: 138 | Iteration: 200 | Loss: 0.689\nEpoch: 138 | Iteration: 250 | Loss: 0.754\nEpoch: 138 | Iteration: 300 | Loss: 0.687\nEpoch: 138 | Iteration: 350 | Loss: 0.878\nEpoch 138: Test accuracy: 0.9369\nEpoch: 139 | Iteration: 50 | Loss: 0.703\nEpoch: 139 | Iteration: 100 | Loss: 0.776\nEpoch: 139 | Iteration: 150 | Loss: 0.662\nEpoch: 139 | Iteration: 200 | Loss: 0.710\nEpoch: 139 | Iteration: 250 | Loss: 0.724\nEpoch: 139 | Iteration: 300 | Loss: 0.698\nEpoch: 139 | Iteration: 350 | Loss: 0.795\nEpoch 139: Test accuracy: 0.9431\nEpoch: 140 | Iteration: 50 | Loss: 0.750\nEpoch: 140 | Iteration: 100 | Loss: 0.649\nEpoch: 140 | Iteration: 150 | Loss: 0.685\nEpoch: 140 | Iteration: 200 | Loss: 0.825\nEpoch: 140 | Iteration: 250 | Loss: 0.654\nEpoch: 140 | Iteration: 300 | Loss: 0.757\nEpoch: 140 | Iteration: 350 | Loss: 0.780\nEpoch 140: Test accuracy: 0.9391\nEpoch: 141 | Iteration: 50 | Loss: 0.651\nEpoch: 141 | Iteration: 100 | Loss: 0.741\nEpoch: 141 | Iteration: 150 | Loss: 0.694\nEpoch: 141 | Iteration: 200 | Loss: 0.669\nEpoch: 141 | Iteration: 250 | Loss: 0.741\nEpoch: 141 | Iteration: 300 | Loss: 0.689\nEpoch: 141 | Iteration: 350 | Loss: 0.643\nEpoch 141: Test accuracy: 0.9440\nEpoch: 142 | Iteration: 50 | Loss: 0.631\nEpoch: 142 | Iteration: 100 | Loss: 0.566\nEpoch: 142 | Iteration: 150 | Loss: 0.711\nEpoch: 142 | Iteration: 200 | Loss: 0.654\nEpoch: 142 | Iteration: 250 | Loss: 0.646\nEpoch: 142 | Iteration: 300 | Loss: 0.720\nEpoch: 142 | Iteration: 350 | Loss: 0.768\nEpoch 142: Test accuracy: 0.9445\nEpoch: 143 | Iteration: 50 | Loss: 0.735\nEpoch: 143 | Iteration: 100 | Loss: 0.716\nEpoch: 143 | Iteration: 150 | Loss: 0.624\nEpoch: 143 | Iteration: 200 | Loss: 0.658\nEpoch: 143 | Iteration: 250 | Loss: 0.794\nEpoch: 143 | Iteration: 300 | Loss: 0.723\nEpoch: 143 | Iteration: 350 | Loss: 0.659\nEpoch 143: Test accuracy: 0.9463\nEpoch: 144 | Iteration: 50 | Loss: 0.757\nEpoch: 144 | Iteration: 100 | Loss: 0.612\nEpoch: 144 | Iteration: 150 | Loss: 0.528\nEpoch: 144 | Iteration: 200 | Loss: 0.670\nEpoch: 144 | Iteration: 250 | Loss: 0.587\nEpoch: 144 | Iteration: 300 | Loss: 0.629\nEpoch: 144 | Iteration: 350 | Loss: 0.504\nEpoch 144: Test accuracy: 0.9473\nEpoch: 145 | Iteration: 50 | Loss: 0.653\nEpoch: 145 | Iteration: 100 | Loss: 0.735\nEpoch: 145 | Iteration: 150 | Loss: 0.669\nEpoch: 145 | Iteration: 200 | Loss: 0.732\nEpoch: 145 | Iteration: 250 | Loss: 0.660\nEpoch: 145 | Iteration: 300 | Loss: 0.644\nEpoch: 145 | Iteration: 350 | Loss: 0.554\nEpoch 145: Test accuracy: 0.9472\nEpoch: 146 | Iteration: 50 | Loss: 0.585\nEpoch: 146 | Iteration: 100 | Loss: 0.712\nEpoch: 146 | Iteration: 150 | Loss: 0.595\nEpoch: 146 | Iteration: 200 | Loss: 0.576\nEpoch: 146 | Iteration: 250 | Loss: 0.734\nEpoch: 146 | Iteration: 300 | Loss: 0.630\nEpoch: 146 | Iteration: 350 | Loss: 0.718\nEpoch 146: Test accuracy: 0.9487\nEpoch: 147 | Iteration: 50 | Loss: 0.632\nEpoch: 147 | Iteration: 100 | Loss: 0.717\nEpoch: 147 | Iteration: 150 | Loss: 0.605\nEpoch: 147 | Iteration: 200 | Loss: 0.591\nEpoch: 147 | Iteration: 250 | Loss: 0.720\nEpoch: 147 | Iteration: 300 | Loss: 0.593\nEpoch: 147 | Iteration: 350 | Loss: 0.563\nEpoch 147: Test accuracy: 0.9502\nEpoch: 148 | Iteration: 50 | Loss: 0.828\nEpoch: 148 | Iteration: 100 | Loss: 0.562\nEpoch: 148 | Iteration: 150 | Loss: 0.664\nEpoch: 148 | Iteration: 200 | Loss: 0.574\nEpoch: 148 | Iteration: 250 | Loss: 0.518\nEpoch: 148 | Iteration: 300 | Loss: 0.518\nEpoch: 148 | Iteration: 350 | Loss: 0.696\nEpoch 148: Test accuracy: 0.9494\nEpoch: 149 | Iteration: 50 | Loss: 0.615\nEpoch: 149 | Iteration: 100 | Loss: 0.617\nEpoch: 149 | Iteration: 150 | Loss: 0.614\nEpoch: 149 | Iteration: 200 | Loss: 0.686\nEpoch: 149 | Iteration: 250 | Loss: 0.592\nEpoch: 149 | Iteration: 300 | Loss: 0.557\nEpoch: 149 | Iteration: 350 | Loss: 0.653\nEpoch 149: Test accuracy: 0.9495\nEpoch: 150 | Iteration: 50 | Loss: 0.616\nEpoch: 150 | Iteration: 100 | Loss: 0.715\nEpoch: 150 | Iteration: 150 | Loss: 0.626\nEpoch: 150 | Iteration: 200 | Loss: 0.690\nEpoch: 150 | Iteration: 250 | Loss: 0.645\nEpoch: 150 | Iteration: 300 | Loss: 0.693\nEpoch: 150 | Iteration: 350 | Loss: 0.633\nEpoch 150: Test accuracy: 0.9492\nModel checkpoint saved to model_checkpoint.pth\n","output_type":"stream"}],"execution_count":9},{"id":"c742d32a-5b4c-4af5-b895-07d2fe44d612","cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:23:00.738263Z","iopub.execute_input":"2025-03-10T00:23:00.738676Z","iopub.status.idle":"2025-03-10T00:23:00.746885Z","shell.execute_reply.started":"2025-03-10T00:23:00.738617Z","shell.execute_reply":"2025-03-10T00:23:00.746029Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"CustomResNet(\n  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (6): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (7): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (8): BasicBlock(\n      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Identity()\n    )\n  )\n  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=256, out_features=10, bias=True)\n)"},"metadata":{}}],"execution_count":10},{"id":"load_test_data_cell","cell_type":"code","source":"# Template code for reading the test file\ndef load_cifar_batch(file):\n    with open(file, 'rb') as fo:\n        batch = pickle.load(fo, encoding='bytes')\n    return batch\n\n# Load the test batch (update the file path if necessary)\ncifar10_batch = load_cifar_batch('/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl')\n\n# Extract images; the test data is in (N x W x H x C) format\nimages = cifar10_batch[b'data']\nprint(f\"Loaded test batch with {images.shape[0]} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:23:00.747802Z","iopub.execute_input":"2025-03-10T00:23:00.748082Z","iopub.status.idle":"2025-03-10T00:23:01.406485Z","shell.execute_reply.started":"2025-03-10T00:23:00.748052Z","shell.execute_reply":"2025-03-10T00:23:01.405722Z"}},"outputs":[{"name":"stdout","text":"Loaded test batch with 10000 images\n","output_type":"stream"}],"execution_count":11},{"id":"create_test_dataset_cell","cell_type":"code","source":"# Create a dataset from the images array\nclass TestDatasetFromArray(Dataset):\n    def __init__(self, images, transform=None):\n        self.images = images\n        if transform is None:\n            # Default transform: convert numpy array to PIL Image, then to tensor, then normalize\n            self.transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n            ])\n        else:\n            self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = self.images[idx]\n        if isinstance(img, np.ndarray):\n            img = img.astype('uint8')\n        if self.transform:\n            img = self.transform(img)\n        return img\n\n# Create the test dataset and dataloader\ntest_dataset = TestDatasetFromArray(images)\nloader_test = DataLoader(test_dataset, batch_size=128, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:23:01.407457Z","iopub.execute_input":"2025-03-10T00:23:01.407791Z","iopub.status.idle":"2025-03-10T00:23:01.414121Z","shell.execute_reply.started":"2025-03-10T00:23:01.407758Z","shell.execute_reply":"2025-03-10T00:23:01.413398Z"}},"outputs":[],"execution_count":12},{"id":"inference_mode_cell","cell_type":"code","source":"# Inference Mode\n# This cell loads a saved model checkpoint (if available) and runs inference on the test dataset.\n# Predictions are then saved to a CSV file.\n\nif os.path.exists(\"model_checkpoint.pth\"):\n    model.load_state_dict(torch.load(\"model_checkpoint.pth\", map_location=device))\n    print(\"Loaded model checkpoint from model_checkpoint.pth\")\nelse:\n    print(\"No checkpoint found. Running inference with untrained model.\")\n\nfrom datetime import datetime\n\nn1 = datetime.now()\nprint(n1)\n\nmodel.eval()\npredictions = []\nwith torch.no_grad():\n    for batch in loader_test:\n        batch = batch.to(device)\n        outputs = model(batch)\n        _, preds = torch.max(outputs, 1)\n        predictions.extend(preds.cpu().numpy().tolist())\n\n\n\nfilename = 'submission_'+str(n1)+'.csv' \n# Save predictions to CSV using pandas\ndf = pd.DataFrame({\"ID\": range(len(predictions)), \"Labels\": predictions})\ndf.to_csv(filename, index=False)\nprint(\"Predictions saved to \"+ filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:23:01.414881Z","iopub.execute_input":"2025-03-10T00:23:01.415137Z","iopub.status.idle":"2025-03-10T00:23:04.642769Z","shell.execute_reply.started":"2025-03-10T00:23:01.415117Z","shell.execute_reply":"2025-03-10T00:23:04.641886Z"}},"outputs":[{"name":"stdout","text":"Loaded model checkpoint from model_checkpoint.pth\n2025-03-10 00:23:01.491108\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-13-0abd5b6c9ad1>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"model_checkpoint.pth\", map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to submission_2025-03-10 00:23:01.491108.csv\n","output_type":"stream"}],"execution_count":13},{"id":"22c685f2-8590-4b7b-8111-4f1c1e089fe6","cell_type":"markdown","source":"## Instructions\n\n1. **Training Section:** Run the training cell to train the model on CIFAR-10. The model checkpoint will be saved as `model_checkpoint.pth`.\n2. **Inference Section:** Ensure the test pickle file is available at `/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl`. Then run the inference cell to generate predictions, which will be saved to `cifar_test_predictions.csv`.\n\nFeel free to adjust hyperparameters (like epochs) as needed.","metadata":{}},{"id":"3408a20c-c92a-416d-9c60-ed909a2ce2d4","cell_type":"code","source":"batch = next(iter(train_loader))\nprint(batch[0].shape)\nprint(batch[1].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:28:18.020158Z","iopub.execute_input":"2025-03-10T00:28:18.020455Z","iopub.status.idle":"2025-03-10T00:28:18.441901Z","shell.execute_reply.started":"2025-03-10T00:28:18.020417Z","shell.execute_reply":"2025-03-10T00:28:18.441003Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 32, 32])\ntorch.Size([128])\n","output_type":"stream"}],"execution_count":21},{"id":"8084522e-9f7c-48e6-a3c1-3ac976606da3","cell_type":"code","source":"batch = next(iter(test_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:31:53.106979Z","iopub.execute_input":"2025-03-10T00:31:53.107275Z","iopub.status.idle":"2025-03-10T00:31:53.120169Z","shell.execute_reply.started":"2025-03-10T00:31:53.107252Z","shell.execute_reply":"2025-03-10T00:31:53.118980Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-85c2dbc8e2d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"],"ename":"NameError","evalue":"name 'test_loader' is not defined","output_type":"error"}],"execution_count":28},{"id":"fe2363a9-d5d8-4ea8-be3b-dc78292a0b84","cell_type":"code","source":"next(iter(loader_test)).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T00:32:20.183772Z","iopub.execute_input":"2025-03-10T00:32:20.184069Z","iopub.status.idle":"2025-03-10T00:32:20.216869Z","shell.execute_reply.started":"2025-03-10T00:32:20.184043Z","shell.execute_reply":"2025-03-10T00:32:20.216213Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"torch.Size([128, 3, 32, 32])"},"metadata":{}}],"execution_count":30},{"id":"cb711fcd-17bd-4d52-a4d2-cae576f2631b","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f2e7bd3a-fcde-4fd0-91e2-218f58c5c18a","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}